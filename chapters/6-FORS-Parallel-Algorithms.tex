\chapter{Fast One-Ring Smoothing: Parallel Algorithms}
\label{ch6}
In the previous two chapters we presented the mathematical grounding and serial algorithms for an improved version of \Fors{t}, as it is currently implemented within the GigaMesh framework, and while it has improved accuracy in regards to the method of weighting the mean function values, it is still yet entirely serial in design, so that unfortunately, its performance suffers greatly under the complexity of modern mesh sizes, which with the current high resolution scanners in use, can grow to be MESH\_SIZES\todoResearch{mesh sizes}. 

In this chapter, we will now explore the parallel version of this as-yet-unpublished algorithm, in order to negotiate any instances of control or data dependency, hopefully discovering manifestations of independent procedures worthy of exploiting with parallel processing, with the goal of improving the performance of the filter when implemented on a system capable of parallel computation.
\todoBackground{Data dependencies}

Following the pattern set by the serial algorithm in Chapter~\ref{ch5}, the parallel algorithm for \Fors{t} also has three main parts. In contrast however, the parallel variant of each of the three serial parts are split further into subroutines, in order to delegate blocks of instructions to separate threads of execution, while protecting critical sections, and avoiding data and control dependencies, so that the threads can be safely executed in parallel.

The following three sections will each focus on producing the parallel variant of one of the three parts of the serial algorithm which was presented in Chapter~\ref{ch5}, by first analyzing the serial algorithm through the lens of parallel programing, next defining the strategy behind the design of the parallel variant, then finally providing the pseudo code definitions to implement that part of the algorithm. 

%
%
%
%
\section{Build Neighborhoods in Parallel}
\label{ch6sBNP}
The sole purpose of Algorithm~\ref{alg:serialBuildNeighborhoods}, as discussed in detail in Section~\ref{ch5sBN}, is to explore every triangular face in the set $\bT$ in order to generate a family of sets of neighborhoods $\bN$, so that subsequent algorithms can recall and iterate over those associations and negate the computational costs incurred by searching the entire mesh each time the membership of a neighborhood must be known. In addition to that goal, because of the nondeterministic nature of the sum of the cardinalities of each neighborhood of $\bN$, the parallel variant of the $\mathit{buildNeighborhoods}$ procedure must also provide a census, a total count of all neighbors in all neighborhoods, so that the next two parts of the parallel variant of \fors{t} algorithm can accurately predict work loads, and evenly delegate portions of the work to independent threads of execution.

%
%
\subsection{Analysis of Serial Algorithm~\ref{alg:serialBuildNeighborhoods}: Build Neighborhoods}
\label{ch6sBNPssASABN}
In this section, we analyze through the lens of parallel programing, the~\nameref{saBN.title} as presented in Section~\ref{ch5sBN}. The strategy we employ involves reading the serial algorithm line by line, building a dependency graph, and as opportunities for exploiting parallelism are discovered, a strategy\todoReword{strategy twice is bad} for maximizing efficiency is devised.
\todoBackground{Dependency Graph}

Starting in line~\ref{sbn1} of Algorithm~\ref{alg:serialBuildNeighborhoods}, we read the declaration of the function \textit{serialBuildNeighborhoods} requiring the entire set of triangular faces $\bT$ as an input, then already in line~\ref{sbn2}, we encounter a loop over each face $\bt$ in $\bT$. In general, concurrency found in the structure of a loop may be completely exploited by a parallel implementation of the same procedure, but only in the absence of loop-carried dependence. So, in order to determine if the iterations of this loop may be computed in parallel, an analysis of every operation in the loop block is required.\todoResearch{Amdahl's Law}
\todoBackground{loop level parallelism, loop-carried and loop-independent}
\todoBackground{static vs volatile memory}

Figure~\ref{fig:sabnDataDependencies} illustrates the data dependencies inherent to each iteration of the loop over the faces $\bt$ in $\bT$, as found in lines~\ref{sbn2}~-~\ref{sbn5} of Algorithm~\ref{alg:serialBuildNeighborhoods}. As denoted by the first three teal colored lines, the first operations are to read the three points $\bp_a$, $\bp_b$, $\bp_c$ from the triangular face $\bt$, stored is the set $\bT$ in static memory, which will never be modified by this, or any other procedure. Also, despite the fact that faces can be adjacent to one another, each face is distinctly defined in the set $\bT$, therefore, each face $\bt$ can be considered independent of each other face. Thus, this first instruction of the loop block is totally free from any control or data dependencies. The next line in the algorithm is a complex instruction, though, so it must be analyzed in parts.

The first part of the instruction on line~\ref{sbn3} uses the point $\bp_a$ as a reference to read the state of $\bN_a$ from the set $\bN$ in volatile memory, as denoted as teal and coral colored lines, respectively. $\bN_a$ is stored in volatile memory, which may potentially be modified by other threads, because it is not known a priori which thread will discover the points which belong to that neighborhood, thus it must be accessible to any and all threads. Next, drawn in sand color, are the two union operations performed in succession between $\bN_a$ and the point $\bp_b$, then $\bp_c$. While these two operations are themselves independent, being performed only on values currently immutable by other procedures, they do rely on having already read the status of $\bN_a$, which indeed does have its own data dependence, hence the indicative coloring. Finally, illustrated as a coral colored double arrow, the updated set $\bN_a$ is stored back into $\bN$, replacing the original set in volatile memory.

\begin{figure}[ht]
	\includestandalone[width=0.8\textwidth]{figures/tikz/sabnDataDependencies}
	{\caption[Data Dependencies in Serial Algorithm~\ref{alg:serialBuildNeighborhoods}: Build Neighborhoods]{An Illustration of the data dependencies found in Algorithm~\ref{alg:serialBuildNeighborhoods}. Operations with data dependencies are in coral color, independent operations are in teal color, and independent operations which reply on operations which have dependencies are in sand color.}\label{fig:sabnDataDependencies}}
\end{figure}

Both reading from, and writing to $\bN_a$ in volatile memory constitutes a critical section in the algorithm, which must be accounted for in the design of the parallel variant of this part of the algorithm. Furthermore, both dependencies lie in a single path of dependence, which means that the entire group of operations must be protected by a guarding mechanism as described in Section~\ref{ch2sPPssMS}. Fortunately, because both dependencies revolve around the same set $\bN_a$, a simple mutex per neighborhood will be sufficient. 

Lines~\ref{sbn4}~and~\ref{sbn5} behave similarly to line~\ref{sbn3}, except they concern other neighborhoods, which in turn, need their own mutexes.\todoBackground{path of dependence} Fortunately, as is modeled with a simple mesh in Figure~\ref{fig:unionsOfSimpleBuildNeighborhoods}, the union operation is performed exactly twice per point per face; that is, once each between the neighborhood of the center point and a neighboring point, for a total of six times per face. By exploiting this pattern and executing the two union operations sequentially within a single mutex, one can mitigate exactly half of the possible collisions in the $\mathit{buildNeighborhoods}$ procedure.

%
%
\subsection{Parallel Variant of Build Neighborhoods}
\label{ch6sBNPssPVBN}
Algorithm~\ref{alg:parallelBuildNeighborhoods} defines the structure and procedures required to implement a parallel variant of the function $\mathit{serialBuildNeighborhoods}$, while ensuring correctness by using a set of mutexes to guard critical sections. The main function $\mathit{parallelBuildNeighborhoods}$ requires only the number of available processors $\rho$, the set of all triangular faces $\bT$, and the cardinality of the set of points $|\bP|$ in order to complete its task. Initially, the ``stride'' is calculated in line~\ref{algPBN.stride} as the portion of the total work load, to which a single processor will be instructed to compute, equal to the problem size divided by the number of available processors; here, the unit of work being an entire triangular face $\bt$. Next, a set of mutexes are created, containing an equal amount of elements as the cardinality of points in $\bP$, to be used individually for each neighborhood. \todoReword{remove if no longer setting $\bM$} Then begins the loop to spawn a ``build'' thread for a quarter of the available processors in the system, with the scalar four being a result of each build thread's need to spawn three additional ``union'' threads. Then all the working threads must synchronize first, before the family of sets of neighborhoods $\bN$ can be finally realized.

In line~\ref{algPBN.build}, we read that along with access to the set of triangular faces $\bT$ and set of mutexes $\fM$, each build thread in Algorithm~\ref{alg:parallelBuildNeighborhoods} also requires an index $\Pi$, which it uses along with the stride $sigma$, to calculate the lower and upper boundaries, $\check{\sigma}$ and $\hat{\sigma}$, for the portion of the work load on which it should compute. Next, it iterates over each face $\bt$ within those stride boundaries, and spawns three union threads; one each for the three points comprising the corners of the current triangular face.

Line~\ref{algPBN.safeUnion} shows that each union thread requires not only the family of sets of neighborhoods $\bN$ and the set of mutexes $\fM$, but also the point which it will consider to be central, and the two points which are the central point's neighbors. With those inputs, each union thread then attempts to lock the mutex which shares the index of both the central point and its corresponding neighborhood, blocking if necessary, in order to safely perform the union operation between the currently known set of neighbors for the central point, and the set of its two newly discovered neighbors; finally, saving the updated neighborhood to volatile shared memory while still protected by the mutex, before unlocking the mutex.
\todoBackground{blocking mutex}

\begin{algorithm}[ht]
	\DontPrintSemicolon
	\SetCommentSty{small}
	\SetKwFor{For}{for}{:}{}
	\SetKwProg{Func}{Function}{}{}
	\SetKwProg{Sub}{Subroutine}{}{}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}

	\Input{the number of available processors $\rho$, \\
		the set of all triangular faces $\bT$, \\
		the cardinality of the set of points $|\bP|$}
	\Output{the family of sets of discovered neighborhoods $\bN$}

	\bigskip
\nl	\Func{parallelBuildNeighborhoods($\rho$, $\bT$, $|\bP|$)}{
\nl		$\sigma \leftarrow |\bT|\mathbin{/}\rho$\tcc*{assuming an integer quotient}
\nl		$\fM \leftarrow \{\mu_1,\,\ldots,\,\mu_{|\bP|}\}$\;
\nl		\For{$\Pi \leftarrow 1$ \KwTo $\rho\mathbin{/}4$}{
\nl			\ProgSty{$\sim$build($\Pi$, $\sigma$, $\fM$, $\bT$)}\;
		}

		\medskip
\nl		\ProgSty{synchronizeThreads()}\;
		\medskip
	}

	\bigskip
\nl	\Sub{build($\Pi$, $\sigma$, $\fM$, $\bT$)}{\label{algPBN.build}
\nl		$\check{\sigma} \leftarrow (\Pi-1)\,\sigma+1$\tcc*{works through its stride}\label{algPBN.stride}
\nl		$\hat{\sigma} \leftarrow \Pi\,\sigma$\;
\nl		\For(\tcc*[f]{$\bt = \left \{\bp_a, \bp_b, \bp_c\right \}$}){$\bt \in \{\bt_{\check{\sigma}},\ldots,\,\bt_{\hat{\sigma}}\}$}{
\nl			\ProgSty{$\sim$safeUnion($\fM$, $\bN$, $\bp_a$, $\bp_b$, $\bp_c$)}\;
\nl			\ProgSty{$\sim$safeUnion($\fM$, $\bN$, $\bp_b$, $\bp_a$, $\bp_c$)}\;
\nl			\ProgSty{$\sim$safeUnion($\fM$, $\bN$, $\bp_c$, $\bp_a$, $\bp_b$)}\;
		}
	}

	\bigskip
\nl	\Sub{safeUnion($\fM$, $\bN$, $a$, $b$, $c$)}{\label{algPBN.safeUnion}
\nl		$\ProcSty{lock}(\mu_a)$\;
\nl		$\bN_a \leftarrow \bN_a \cup \{b,\,c\}$\;
\nl		$\ProcSty{unlock}(\mu_a)$\;
	}
	\caption{Parallel algorithm for building the family of sets of all members of each neighborhood discovered in the mesh \label{alg:parallelBuildNeighborhoods}}
\end{algorithm}%
\nomenclature[]{$\rho$}{the number of available processors}%
\nomenclature[]{$\sigma$}{the ``stride'', the size of a block of work intended for a single processor, equal to the problem size divided by the number of available processors}%
\nomenclature[]{$\check{\sigma}$}{the index of the beginning of a stride}%
\nomenclature[]{$\hat{\sigma}$}{the index of the beginning of a stride}%
\nomenclature[]{$\Pi$}{the index representing a single processor}%
\nomenclature[]{$\fM$}{a set of mutexes}%
\nomenclature[]{$\mu_v$}{a specific mutex}%
\nomenclature[]{$\sim process()$}{a process to be run in a new thread}%

%
%
\subsection{Parallel Recursive Census Neighborhoods}
\label{ch6sBNPssPRCN}
The motivation behind Algorithm~\ref{alg:parallelRecursiveCensusNeighborhoods} is that the next two parts of parallel algorithm for \fors{t} perform individual computations for each neighbor in the family of sets of neighborhoods $\bN$, and so require a total count of all neighbors in all neighborhoods in order to accurately predict and evenly delegate portions of the work load to each independent thread of execution. Additionally, because of the nondeterministic nature of the cardinality of neighborhoods in acquired \tdd{}, therefore also the total sum of all cardinalities of neighborhoods in $\bN$, the parallel variant of the function $\mathit{serialBuildNeighborhoods}$ must also provide a census of total membership, defined as
%
\begin{equation}
	\hat{n} := \sum\limits_{v=1}^{|\bN|}{|\bN_v|}
	\label{eq:censusNeighborhoods}
\end{equation}%
\nomenclature[]{\hat{n}}{}%

While a sum of all cardinalities could be accumulated by each thread in  Algorithm~\ref{alg:parallelBuildNeighborhoods}, unlike in the serial version, it would very inefficient\footnote{It is possible that in some programming languages, the underlying data structure may automatically keep a record of the size of membership in $\bN$. In that case, one would simply ignore Algorithm~\ref{alg:parallelRecursiveCensusNeighborhoods} and instead assign that generated value to $\hat{n}$.} due to the added overhead of collisions with the guarding mechanism required to protect the shared value of the total sum.  Alternatively, algorithm~\ref{alg:parallelRecursiveCensusNeighborhoods} only requires as inputs, the family of sets of neighborhoods $\bN$, and the number of processors available in the system, but is highly parallel and performs at the quick rate of $O(log_2(n))$, so when using a system with multiple processors, it is a much more efficient alternative to calculating the sum implicitly while building $\bN$.

\begin{figure}[ht]
	{\includegraphics[width=1.0\linewidth]{example-image-16x9.png}}
	%\includestandalone[width=0.8\textwidth]{figures/tikz/sabnDataDependencies}
	{\caption[wefwefwef]{sdfsdf}\label{fig:wfwef}}
\end{figure}

As Algorithm~\ref{alg:parallelRecursiveCensusNeighborhoods} is a recursive algorithm, it is best described by dividing it into its three distinct parts. The first part, ``the termination clause'', only executes when the cardinality of the current subset of $\bN$ is two or less\footnote{This conditional operator allows for unit subsets of only a single member, which is possible anytime one processes a set with a cardinality other than one of the positive powers of two, \{2, 4, 8,\ldots , 1048579, \ldots\}, so most of the time.}, then finally returning the sum\footnote{This ignores the trivial case where the cardinality of the full family of sets $\bN$ is less than three, thus never being processed by the \textit{parallelSum} subroutine. If that is something required, only a simple modification similar to the conditional statement found in line~\ref{algPRCN.onlyFirstCall} must be made.} of the members as $\hat{n}$. In all other cases, when the cardinality of the current subset of $\bN$ is greater than two, the second part, ``working towards the termination state'', begins. 
\todoBackground{recursive algorithms, three parts}

The second part of this recursive strategy describes summing in parallel, the cardinality or value of each adjacent pair of members in the current subset of $\bN$, then saving the sum of each addition in a new ordered subset of integers $\widetilde{\bN}$, which will have half the cardinality of the current subset. To that end, the stride is calculated using one half of the cardinality of $\bN$ as the total work load, to be divided among all of the available processors. Next, a new thread is spawned for each processor in order to execute the subroutine \textit{parallelSum} to process each portion of the work load in parallel.

The subroutine \textit{parallelSum} requires as inputs: a process index $\Pi$, the stride $sigma$, and the current subset of neighborhood cardinalities being processed\footnote{or if this is the first call, the family of sets of neighborhoods $|\bN|$}. First it calculates the lower and upper boundaries, $\check{\sigma}$ and $\hat{\sigma}$, for the portion of the work load on which it should compute. Next, it iterates over every-other index within the stride boundries, in order to return the sum of the cardinalities of each even-odd pair of neighborhoods. While it may appear that this is an accumulation threatening a race-condition, because each sum is stored as its own unique value to be used in subsequent additions, albeit in the same set, there is no need to protect these operations with guarding mechanisms, thus contributing greatly to the overall efficiency of the algorithm.

In the third and final part, ``the recursive call'', the function \textit{parallelRecursiveCensusNeighborhoods} must first wait for all the threads executing instances of \textit{parallelSum} to synchronize before  calling itself, using the new subset $\widetilde{\bN}$ as a parameter instead of the original $\bN$. In each iteration every available processor is utilized, and the cardinality of $\bN$ is reduced by half, quickly approaching the termination clause, while requiring no guarding mechanisms, and even without memory recycling, requiring less than twice the total memory required than just storing $\bN|$.

\begin{algorithm}[ht]
	\algotitle{Parallel Algorithm for Recursively Counting a Census of all Neighborhoods}{paRCN.title}
	\DontPrintSemicolon
	\SetCommentSty{small}
	\SetKwFor{For}{for}{:}{}
	\SetKwProg{Func}{Function}{}{}
	\SetKwProg{Sub}{Subroutine}{}{}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}

	\Input{the number of available processors $\rho$, \\
		the family of sets of neighborhoods $\bN$}
	\Output{the count of all neighbors in all neighborhoods $\hat{n}$}

	\bigskip
\nl	\Func{parallelRecursiveCensusNeighborhoods($\rho$, $\bN$)}{
\nl		\eIf{$|\bN| \leq 2$}{
\nl			$\hat{n} \leftarrow \sum_{i=1}^{|\bN|}\bN_i$\;
		}{%Else
\nl			$\sigma \leftarrow |\bN|\mathbin{/}(2\,\rho)$\;
\nl			\For{$\Pi \in \{1,\ldots,\rho\}$}{
\nl				\ProgSty{$\sim$parallelSum($\Pi$, $\sigma$, $\bN$)}\;
			}
			\medskip
\nl			\ProgSty{synchronizeThreads()}\;
			\medskip
\nl			\ProgSty{parallelRecursiveCensusNeighborhoods($\rho$, $\widetilde{\bN}$)}\;
		}
	}

	\bigskip
\nl	\Sub{parallelSum($\Pi$, $\sigma$, $\bN$)}{
\nl		$\check{\sigma} \leftarrow (\Pi-1)\,\sigma+1$\;
\nl		$\hat{\sigma} \leftarrow 2\,\Pi\,\sigma$\;
\nl		\For{$v \in \{\check{\sigma},\,\check{\sigma}\sps{}2,\,\check{\sigma}\sps{}4,\ldots,\,\hat{\sigma}\}$}{
\nl			\eIf(\tcc*[f]{only occurs in first call\footnotemark}){$\bN$ is a family of sets}{\label{algPRCN.onlyFirstCall}
\nl				$\widetilde{\bN_v} \leftarrow |\bN_v| + |\bN_{\sxpx{v}{1}}|$\;
			}{%else
\nl				$\widetilde{\bN_v} \leftarrow \bN_v + \bN_{\sxpx{v}{1}}$\;
			}
		}
	}
	\caption{Parallel algorithm for recursively counting a census of all neighbors in all neighborhoods \label{alg:parallelRecursiveCensusNeighborhoods}}
\end{algorithm}%
\nomenclature[]{$\hat{n}$}{a census, the count of all neighbors in all neighborhoods}%
\nomenclature[]{$\widetilde{\bN}$}{a subset of $\bN$ used temporarily in Algorithm~\ref{alg:parallelRecursiveCensusNeighborhoods}}%
\todoReword{mention that non-existent neighborhoods should be treated as having a cardinality of 0}
\footnotetext{It is important to notice in line~\ref{algPRCN.onlyFirstCall}, the distinction between the family of sets of neighborhoods $\bN$ in the first call of \textit{parallelRecursiveCensusNeighborhoods}, with that of the set of integers accumulating in the subset $\widetilde{\bN}$ in subsequent calls.}

%
%
%
%
\section{Calculating Edge Lengths in Parallel}
\label{ch6sCELP}
In this section we will focus on producing the parallel variant of  Algorithm~\ref{alg:serialCalculateEdgeLengths}, as presented in Chapter~\ref{ch5}, which has the goal of building a set of pre-calculated edge lengths $\bE$, as well as determining the global minimum edge length $\gelm$; both essential parameters of Algorithm~\ref{alg:serialConvolveFilter}. First we will analyze the serial algorithm through the lens of parallel programing, to be followed by defining the strategy behind the design of the parallel variant, then finally providing the pseudo code definitions to implement Algorithm~\ref{alg:parallelCalculateEdgeLengths}.


%
%
\subsection{Analysis of Serial Algorithm~\ref{alg:serialCalculateEdgeLengths}: Calculate Edge Lengths}
\label{ch6sCELPssASACEL}
In this section, we analyze the~\nameref{saCEL.title} as presented in Section~\ref{ch5sCEL}, through the lens of parallel programing. The strategy we employ involves reading the serial algorithm line by line, building a dependency graph, and as opportunities for exploiting parallelism are discovered, a strategy\todoReword{strategy twice is bad} for maximizing efficiency is devised.

Already in line~\ref{scel1} of Algorithm~\ref{alg:serialCalculateEdgeLengths}, we encounter a loop over each point $\bp_v$ in $\bP$. In order to determine if the iterations of the loop may instead be computed in parallel, an analysis of every operation in the loop block is required, which in this case, includes lines~\ref{scel2}-~\ref{scel5}. The first internal line,~\ref{scel2}, starts another loop over each point $\bp_i$ in $\bN_v$. \todoReword{why important to know ?} As we have seen very clearly in Figure~\ref{fig:neighborhoods}, the cardinality of each individual neighborhood can not be predicted for irregular, triangle meshes, like those typical of acquired \tdd{}. However, that becomes less important because it is possible to know the total count of neighbors of in all neighborhoods $\hat{n}$, by counting the size of each neighborhood a posteriori, as is done in Algorithm~\ref{alg:parallelRecursiveCensusNeighborhoods}. With $\hat{n}$, we may unroll this pair of loops and instruct a calculable number of threads to process the loop block in parallel. If the average number of neighbors per point is six\todoResearch{average number of neighbors}, then the number of edge lengths to be calculated and stored will be six times as large as the cardinality of $\bP$.

\begin{figure}[ht]
	\includestandalone[width=0.8\textwidth]{figures/tikz/sacelDataDependencies}
	{\caption[Data Dependencies in Serial Algorithm~\ref{alg:serialCalculateEdgeLengths}: Calculate Edge Lengths]{An Illustration of the data dependencies found in Algorithm~\ref{alg:serialCalculateEdgeLengths}. Operations with data dependencies are in coral color, independent operations are in teal color, and independent operations which reply on operations which have dependencies are in sand color.}\label{fig:sacelDataDependencies}}
\end{figure}

Line~\ref{scel3} of Algorithm~\ref{alg:serialCalculateEdgeLengths} is the $\ellstar$ operation, the most costly operation performed by \fors{t}, due to use of the $\sqrt{(\cdot)}$ operation. Therefore, unlike with the union operation in Algorithm~\ref{alg:parallelBuildNeighborhoods}, it is of paramount importantance that we avoid any unnecessary duplication of the $\ellstar$ operation. In Algorithm~\ref{alg:serialBuildNeighborhoods}, each pair of adjacent points are represented twice, being indexed once from both directions, and while calculating the length both times is exactly what we want to avoid, it is a design choice related to Section\todoReference{memory vs speed}\todoBackground{memory vs speed} whether to store the length twice. The benefit of storing the set of adjacent points in this way, it that it creates an implicit reverse lookup-table which is well documented\todoCitation{multiples, reverse lookup table} for increasing the speed of computations, and further simplifies the complexity of indexing the values, conversely to save memory, one could store the value only once by implementing the control structures for detecting if an edge length has already been saved \todoReference{more details about this method in future work}, then when retrieving the values, one could search for the edge length required at the cost of compute time. We have user-defined to detail the first, speedier method.

%
%
\subsection{Parallel Variant of Calculate Edge Lengths}
\label{ch6sCELPssPVCEL}
Algorithm~\ref{alg:parallelCalculateEdgeLengths} is the parallel algorithm for calculating all the edge lengths between each pair of adjacent points in the mesh. It requires the knowledge of and access to the number of available processors in the system $\rho$, the set of all points $\bP$, the family of sets of neighborhoods $\bN$, the average size of every neighborhood in $\bar{n}$, and the count of all neighbors in all neighborhoods $\hat{n}$, and produces as an output, the set of pre-calculated edge lengths $\bE$, and the global minimum edge length $\gelm$; both instrumental to the calculation of \Fors{t}. This algorithm has been split further into three subroutines in order to maximize efficiency by facilitating the balance of work loaded in parallel onto each processor.

\begin{algorithm}[ht]
	\algotitle{Parallel Algorithm for Calculating Edge Lengths}{paCEL.title}
	\DontPrintSemicolon
	\SetCommentSty{small}
	\SetKwFor{For}{for}{:}{}
	\SetKwProg{Sub}{Subroutine}{}{}
	\SetKwProg{Func}{Function}{}{}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}

	\Input{the number of available processors $\rho$, \\
		the set of all points $\bP$, \\
		the family of sets of neighborhoods $\bN$, \\
		the average size of every neighborhood in $\bar{n}$, \\
		the count of all neighbors in all neighborhoods $\hat{n}$}
	\Output{the set of pre-calculated edge lengths $\bE$, \\
		the global minimum edge length $\gelm$}

	\bigskip
\nl	\Func{parallelCalculateEdgeLengths($\rho$, $\bP$,\,$\bN$,\,$\hat{n}$)}{
\nl		$\sigma \leftarrow \hat{n}\mathbin{/}\rho$\label{algPCELellstar}\;
\nl		\For{$\Pi \in \{1,\,\ldots,\,\left \lceil\rho/\bar{n}\right\rceil\}$}{
\nl			\ProgSty{$\sim$calculateLengths($\Pi$, $\sigma$, $\mu$, $\bP$,\,$\bN$)}\;
		}

		\medskip
\nl		\ProgSty{synchronizeThreads()}\;
		\medskip
	}

	\bigskip
\nl	\Sub{calculateLengths($\Pi$, $\sigma$, $\mu$, $\bP$,\,$\bN$)}{
\nl		$\check{\sigma} \leftarrow (\Pi-1)\,\sigma+1$\;
\nl		$\hat{\sigma} \leftarrow \Pi\,\sigma$\;
\nl		\For{$\bp_v \in \{\bp_{\check{\sigma}},\ldots,\,\bp_{\hat{\sigma}}\}$}{
\nl			\For{$\bp_i \in \bN_v$}{
\nl				\ProgSty{$\sim$safeEdgeLengthCalculation($\mu$, $\bE$, $\gelm$, $\bp_v$, $\bp_i$)}\;
			}
		}
	}

	\bigskip
\nl	\Sub{safeEdgeLengthCalculation($\mu$, $\bE$, $\gelm$, $\bp_v$, $\bp_i$)}{
\nl		$\bE_{\sv{i}} \leftarrow |\bp_i - \bp_v|$\tcc*[r]{$\ellstar$, Eq:~\ref{eq:localMinimumEdgeLength}}
\nl		\If(\tcc*[f]{heuristic only\footnotemark}){$\bE_{\sv{i}} < \gelm$}{\label{algPCELhcs}
\nl			$\ProcSty{lock}(\mu)$\;
\nl			$\gelm \leftarrow \min\left \{\gelm,\,\bE_{\sv{i}}\right \}$\label{algPCELgelm}\tcc*[r]{Eq:~\ref{eq:globalMinimumEdgeLength}}
\nl			$\ProcSty{unlock}(\mu)$\;
		}
	}
	\caption{Parallel algorithm for calculating all the edge lengths between each pair of adjacent points in the mesh\label{alg:parallelCalculateEdgeLengths}}
\end{algorithm}
\footnotetext{While it is true that we are attempting to avoid any unnecessary edge length calculations or mutex locks, the hidden message in this line is honestly just a happy accident.}
\todoBackground{lock/unlock/mutex}
\todoReword{$\bE_{\sv{i}}$ is a unique address, so no mutex required}
\todoBackground{future work, can calculate average neighborhood size in alg.1 or 4}

The initial function, \textit{parallelCalculateEdgeLengths}, requires all of the inputs listed in the previous paragraph, then calculates the stride with the problem size equating to the count of all neighbors in all neighborhoods $\hat{n}$. Next, the set of mutexes is prepared, and the loop iterating over a portion of the count of processors is encountered, calling the subroutine \textit{calculateLengths} in each iteration. Because each thread will be expected to spawn an additional thread for each pair of adjacent points found in every neighborhood in its designated stride of the work load, the greater portion of processors, scaled to the average neighborhood size of the mesh $\bar{n}$, is held in reserve for those spawned threads to be able to run in parallel. Finally, all the working threads must synchronized before the set of pre-calculated edge lengths $\bE$ before the final value of $\gelm$ may be used.

The subroutin \textit{calculateLengths}e requires an index $\Pi$, which it uses along with the stride size $sigma$, to calculate the lower and upper boundaries, $\check{\sigma}$ and $\hat{\sigma}$, for the stride of the total problem which it should compute. Next, the function iterates over each point $\bp_v$ within its stride boundaries, as well as each neighbor $\bp_i$ in the current center point's neighborhood $\bN_v$, spawning new threads to execute instances of the \textit{safeEdgeLengthCalculation} procedure in each iteration.

The subroutine, \textit{safeEdgeLengthCalculation}, requires access to the set of edge lengths $\bE$ being collaboratively populated by each other instance of itself, as well as the single mutex $mu$ used to guard the final reading of and writing to the shared value of $\gelm$. Naturally, also required are the coordinates of the two points, between which the distance is being calculated. In line~\ref{algPCELellstar}, the first line of the function, the L2-norm of the difference between the two points is already calculated; this is the $\ellstar$ operation. There are two race conditions in line~\ref{algPCELgelm} which must be avoided, however it would be incredibly inefficient to have all $(\bar{n}-\rho)\mathbin{/}\bar{n}$ threads attempt to lock the same, single mutex $\mu$. The solution is the heuristic conditional statement in line~\ref{algPCELhcs}, testing the worthiness of incurring the cost of attempting to lock the shared mutex guarding the reads and writes to $\gelm$. We call this conditional statement heuristic, in order to call attention to the fact that it may give an inaccurate result due to race conditions between the threads operating in parallel. \todoResearch{just how much time can be saved with this heuristic?}

In the next section, we will see how the pre-calculations of Algorithms~\ref{alg:parallelBuildNeighborhoods}, ~\ref{alg:parallelRecursiveCensusNeighborhoods}, and ~\ref{alg:parallelCalculateEdgeLengths} contribute to the speed and scalability of the main procedure, alongside the modifications to exploit the concurrency inherent to the filter.

%
%
%
%
\section{Convolving the Filter in Parallel}
\label{ch6sCFP}
Finally, we examine the serial Algorithm~\ref{alg:serialConvolveFilter}, which includes the principle loop to convolve \Fors{t}. As a prerequisite, one must have already run the previous three parallel algorithms, as detailed in Sections~\ref{ch5sPAssBN} and~\ref{ch5sPAssCEL}, in order to generate the input parameters which will be used in the parallel variant of this algorithm.

%
%
\subsection{Analysis of Serial Algorithm~\ref{alg:serialConvolveFilter}: Convolve Filter}
\label{ch6sCFPssASACF}
In this section, we analyze the~\nameref{saCF.title} through the lens of parallel programing.

In the first line of the serial Algorithm~\ref{alg:serialConvolveFilter}, we encounter a loop which runs for a user-defined number of convolutions. In order to determine if one can parallelize the entire loop, once must closely analyze each operation within the loop block; searching for loop-carried control or data dependencies. In line~\ref{algSCFlastLine}, the last line of the loop block, the set of function values $\bF$ is replaced by the newly computed set of \wmfv{s} $\bF'$, before starting the next convolution. This models exactly the definition of a loop-carried data dependency, therefore it would be impossible to calculate this loop entirely\footnote{What may be possible, would be to pipeline calculations for selected points between convolutions, since calculation only actually require the new function values of its neighbors, not the entire set. However, given the effort required to compute a single convolution of \fors{t} on typical acquired \tdd{}, it is unlikely that any additional speedup will be realized using such a technique, computing with the GPGPUs commercially available today.} in parallel.

In the second and third lines of Algorithm~\ref{alg:serialConvolveFilter}, we encounter two more loops iterating over each point in $\bP$, and then each neighborhood $\bN_v$ associated with those points. We have seen this pattern before;\todoReword{talk about pattern when we see it first}together, these two loops iterate over the family of sets of neighborhoods $\bN$, and indeed can be unrolled and computed in parallel barring any occurrences of dependicies within the loop block. Lines~\ref{algSCFalpha} -~\ref{algSCFcheckf} only  generate new terms which are unique to the current iteration, using only original values which are not changed by any other thread, therefore are all prime candidates for exploiting concurrency in the loop. Line~\ref{algSCFjloop} starts a new loop over just two values, and while it would be possible to compute both halves of the values the loop in parallel, at this point in the computation, we would expect all processors to already be in use calculating the other iterations, so dedicating the resources to spawn new threads here would likely net a loss in efficiency, so we will simply unroll the loop and compute the four new terms in serial, per thread.

Lines~\ref{algSCFtildef} and~\ref{algSCFtildea} both accumulate values in shared memory which will be accessed in parallel, on average $\bar{n}$ times, setting up race conditions and threatening the correctness of the algorithm. Because of the limited number of possible collisions, we will treat these two lines as a single critical section, protecting each pair with a shared mutex as we did for the paired union operations in Algorithm~\ref{parallelBuildNeighborhoods}. However, in contrast, these mutexes will have a limited scope, not being shared globally, but only shared within a local neighborhood.

In line~\ref{algSCFfprimev}, the values $\tilde{f_v}$ and $\tilde{A_v}$, having been accumulated for each neighbor in $\bN_V$, are used to calculate the \wmfv{} for the entire geodesic disc $f'_v$, but before their values are read, all threads working on that neighborhood must be synchronized to ensure the correct values are used. As thread synchronization is quite expensive to computation time\todoReference{syncing is expensive}, it is imperative that only those threads working on the neighborhood are synchronized, and not all threads, as is necessary in most other parts of this algorithm, including between the next two lines.

In lines~\ref{algSCF2ndlastline} and~\ref{algSCFlastline}, all the values of $f'_v$ from each neighborhood in $\bN$ are collected into the new set $\bF'$,  then either returned as the final result of the filter's convolutions, or used as the scalar field of function values in the next iteration of the filter. Saving the values to the new set may be done in parallel, because each value will remain unique in the set, however, before the new set may be used in either way all threads of execution must have time to synchronize in order to ensure that only the correct values are used.

\begin{figure}[ht]
	\includestandalone[width=0.8\textwidth]{figures/tikz/sacfDataDependencies}
	{\caption[Data Dependencies in Serial Algorithm~\ref{alg:serialConvolvefilter}: Calculate Edge Lengths]{An Illustration of the data dependencies found in Algorithm~\ref{alg:serialConvolvefilter}. Operations with data dependencies are in coral color, independent operations are in teal color, and independent operations which reply on operations which have dependencies are in sand color.}\label{fig:sacfDataDependencies}}
\end{figure}

Now that we have analyzed, the serial Algorithm~\ref{alg:serialConvolveFilter}, and discovered opportunities where exploiting the loop-level concurrency will benefit the efficiency of \fors{t}, let us not hesitate to create the parallel variant.

%
%
\subsection{Parallel Variant of Convolve Filter}
\label{ch6sCFPssPRCN}
Algorith~\ref{alg:parallelConvolveFilter}, is split into three functions. The first function \textit{parallelConvolveFilter} requires several inputs which include: the number of available processors $\rho$, the set of all points $\bP$, the family of sets of neighborhoods $\bN$, the count of all neighbors in all neighborhoods $\hat{n}$, the set of pre-calculated edge lengths $\bE$, the global minimum edge length $\gelm$, the set of function values $\bF$, and the user-defined number of convolutions $\tau$.

In the first line, line~\ref{algPCFtauloop}, begins the loop iterating for the user-defined number of times $\tau$. Initially, the stride size $\sigma$ is calculated with the work load computed as one for each point defining a geodesic disc $\bO$, which is equal to the cardinality of points in the mesh $|\bP|$, plus one for each neighboring point in every neighborhood $\hat{n}$. Next, another loop is begun, which iterates over a portion of the available processors in the system, as we did in Algorithm~\ref{alg:parallelCalculateEdgeLengths}, reserving resources for the rest of the threads that must be spawned in order to compute in parallel, the \wmfv{} over an entire neighborhood $f'_v$. In each iteration of this loop, a new thread is spawned to execute the function \textit{safeAccumulateGeoDiscMean}, passing to it as parameters: a processor's index $\Pi$, the stride length $\sigma$, the new scalar field which is to be populated with calculated \wmfv{s} from each neighborhood $\bF'$, and the other general information defining and describing the mesh, $\bP$, $\bN$, $\bE$, $\gelm$, $\bF$. At the conclusion of the loop block, all threads are synchronized before either the new scalar field of \wmfv{s} $\bF'$ are used as the function values upon which the next convolution of the filter is based, or the convolutions end and $\bF'$ is returned as the final result.

In line~\ref{algPCFsagdmCall}, the function \textit{safeAccumulateGeoDiscMean} is called for each processor, not reserved for sector-wise computations. That function begins by using its designated index $\Pi$, to compute the lower and upper boundaries, $\check{\sigma}$ and $\hat{\sigma}$, for the stride of the problem on which it should compute. Next, the function iterates over each point $\bp_v$ within its stride boundaries, creating a new set of mutexes $\bM$, containing one $\mu$ per neighbor in currently neighborhood $\bN_v$, before beginning another loop. This inner loop iterates over each neighbor $\bp_i$ in the neighborhood $\bN_v$, and spawns new threads to execute instances of the \textit{safeAccumulateSectorMean} subroutine. After the loop of $\bN_v$ finishes, the threads which were used to calculate the \wmfv{s} for each circle sector are synchronized, before their values are used in the calculation of $f'_v$, the \wmfv{} at point $\bp_v$. While $\bF'$ is a set shared among all the threads, the assignment of $f'_v$ does not require a mutex to guard against race conditions, because during any given convolution of the filter, only one single thread will ever compute and attempt to store the value of $f'_v$ at the location $\bF'_v$.

In line~\ref{algPCFsasmCall}, a new thread is spawned to execute an instance of the function \textit{safeAccumulateSectorMean} for each sector in the current neighborhood. In the first nine lines of \textit{safeAccumulateSectorMean} culminate in the computation of $\check{f}$, the \wmfv{}, for the current circle sector being processed. Because the goal is to accumulate these new values, as well as the area for each sector, in order to be used in the calculation of the weighted mean over the entire geodesic disc, we chose to guard the shared memory values $\tilde{f}_v$ and $\tilde{A}_v$ using mutexes, risking on average a small number of collisions, only $\bar{n}$ per neighborhood. An alternative solution, which we choose not to implement here because it would cost $2\cdot\hat{n}$ more floating point values worth of memory, is to save each intermediate value separately, then perform the averaging calculation on those values after having synchronized all the threads.

\begin{algorithm}[ht]
	\algotitle{Parallel Algorithm for Convolving the Filter}{paCF.title}
	\DontPrintSemicolon
	\SetCommentSty{small}
	\SetKwFor{For}{for}{:}{}
	\SetKwProg{Func}{Function}{}{}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}

	\Input{the number of available processors $\rho$, \\
		the set of all points $\bP$, \\
		the family of sets of neighborhoods $\bN$, \\
		the count of all neighbors in all neighborhoods $\hat{n}$, \\
		the average size of every neighborhood in $\bar{n}$, \\
		the set of pre-calculated edge lengths $\bE$, \\
		the global minimum edge length $\gelm$, \\
		the set of function values $\bF$, \\
		the user-defined number of convolutions $\tau$}
	\Output{the set of one-ring \wmfv{s} $\bF'$}

	\bigskip
	\linespread{1}\selectfont
\nl	\Func{parallelConvolveFilter($\rho$, $\bP$, $\bN$, $\hat{n}$, $\bar{n}$, $\bE$, $\gelm$, $\bF$, $\tau$)}{
\nl		\For{$t\leftarrow 1\;\KwTo\;\tau$}{\label{algPCFtauloop}
\nl			$\sigma \leftarrow (|\bP|+\hat{n})\mathbin{/}\rho$\;
\nl			\For{$\Pi \in \{1,\ldots,\left \lceil\rho/\bar{n}\right\rceil\}$}{
\nl				\ProgSty{$\sim$safeAccumulatesGeoDiscMean($\Pi$, $\sigma$, $\bP$, $\bN$, $\bE$, $\gelm$, $\bF$, $\bF'$)}\;\label{algPCFsagdmCall}
			}
\nl			\ProgSty{synchronizeThreads()}\;
			\medskip

\nl 		\If(\tcc*[f]{smooth newest values every convolution}){$t < \tau$}{$\bF \leftarrow \bF'$}
		}
\nl 	\KwRet $\bF'$\;
	}

	\bigskip
\nl	\Func{safeAccumulatesGeoDiscMean($\Pi$, $\sigma$, $\bP$, $\bN$, $\bE$, $\gelm$, $\bF$, $\bF'$)}{
\nl		$\check{\sigma} \leftarrow (\Pi-1)\,\sigma+1$\;
\nl		$\hat{\sigma} \leftarrow \Pi\,\sigma$\;
\nl		\For{$\bp_v \in \{\bp_{\check{\sigma}},\ldots,\,\bp_{\hat{\sigma}}\}$}{
\nl			$\fM \leftarrow \left \{\mu_1,\dots,\mu_{|\bN_v|}\right \}$\;
\nl			\For{$\bp_i \in \bN_v$}{
				\smallskip
\nl				\ProgSty{$\sim$safeAccumulateSectorMean($\fM$, $\bE$, $\gelm$, $\bp_v$, $\bp_i$, $\bF$, $\tilde{f}_v$, $\tilde{A}_v$)}\;\label{algPCFsasmCall}
			}
\nl			\ProgSty{synchronizeThreads(\{$\check{\sigma}$, \ldots, $\hat{\sigma}$\})}\;

			\medskip
\nl			$\bF'_v \leftarrow \tilde{f}_v\mathbin{/}\tilde{A}_v$\tcc*[r]{as $f'_v$, Eq:~\ref{eq:meanFuncValAtPv}}
		}
	}

	\bigskip
\nl	\Func{safeAccumulateSectorMean($\fM$, $\bE$, $\gelm$, $\bp_v$, $\bp_i$, $\bF$, $\tilde{f}_v$, $\tilde{A}_v$)}{
		\linespread{1.5}\selectfont
\nl		$\kern-0.5pt\alpha \leftarrow cos^{-1}$
		\begin{Large}
			$\kern-6pt\left (\frac{\bE_c^2\,+\,\bE_b^2\,-\,\bE_a^2}{2\,\cdot\,\bE_c\,\cdot\,\bE_b}\right )$\tcc*[r]{Eq:~\ref{eq:alphaFromEdgeLengths}}
		\end{Large}
		\linespread{1.2}\selectfont
\nl		$\kern0.00pt\beta \leftarrow (\pi - \alpha)\mathbin{/}2$\tcc*[r]{Eq:~\ref{eq:betaFromHalfAlpha}}
\nl		$\kern-1.5ptA \leftarrow \Big(\gelm\,\Big)^2\kern-4pt\cdot\alpha\mathbin{/}2$\tcc*[r]{Eq:~\ref{eq:circularSectorArea}}
\nl		$\kern1.00pt\check{\ell} \leftarrow \big(4\cdot\gelm\cdot\sin(\alpha\mathbin{/}2)\big)\mathbin{/}3\,\alpha$\tcc*[r]{Eq:~\ref{eq:distToCoG}}
\nl		$\kern1.00pt\zeta \leftarrow \gelm\mathbin{/}\sin(\beta)$\tcc*[r]{Eq:~\ref{eq:zeta}}
\nl		\For{$j \in {1,2}$}{
\nl			$\tilde{\ell}_j \leftarrow \zeta\mathbin{/}\bE_j$\tcc*[r]{Eq:~\ref{eq:distanceIForInterpolationalgSCFlastLine},~\ref{eq:distanceIp1ForInterpolation}}
\nl			$f'_j \leftarrow f_0\cdot(1 - \tilde{\ell}_j) + f_j\cdot\tilde{\ell}_j$\tcc*[r]{Eq:~\ref{eq:interpolatedFi},~\ref{eq:interpolatedFip1}}
		}
\nl		$\check{f} \leftarrow f_0\cdot(1 - \check{\ell}) + \big((f'_1 + f'_2)\cdot\check{\ell}\big)\mathbin{/}2$\tcc*[r]{Eq:~\ref{eq:weightedMeanAtCoGatSector}}
		\linespread{1.0}\selectfont
\nl		$\ProcSty{lock}(\mu)$\;
\nl		$\kern2.0pt\tilde{f}_v \leftarrow \tilde{f}_v + A\cdot\check{f}$\tcc*[r]{Eq:~\ref{eq:meanFuncValAtPv}}
\nl		$\kern0.0pt\tilde{A}_v \leftarrow \tilde{A}_v + A$\tcc*[r]{Eq:~\ref{eq:meanFuncValAtPv}}
\nl		$\ProcSty{lock}(\mu)$\;
	}

	\caption{Parallel algorithm for convolving \Fors{t}\label{alg:parallelConvolveFilter}}
\end{algorithm}%
%\todoReword{ensure counter is real value}
\todoReword{make sure sigmas include control htereads too}
\todoReword{Alg3,6 output}

The final result is that the \wmfv{s} $\tilde{f'}_v$ and areas $\tilde{A}_v$ of each circle sector are calculated in parallel, then those values are averaged, also in parallel, over the total area of the geodesic disc centered at each $\bp_v$, and collected in the set $\bF'$, completing a single convolution of \Fors{t}.

%
%
%
%
\section{Summary}
\ldots
