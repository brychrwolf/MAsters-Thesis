\chapter{Fast One-Ring Smoothing: Parallel Algorithms}
\label{ch6}
 the previous chapter we presented the mathematical grounding for an improved version of \Fors{t} as it is currently implemented within the GigaMesh framework, and while it has improved accuracy in regards to the weights, it is still yet entirely serial in design, so that unfortunately, its performance suffers greatly under the complexity of modern mesh sizes, which with the current high resolution scanners in use, can grow to be MESH\_SIZES\todoResearch{mesh sizes}. In this chapter, we will now explore the serial version of this as-yet-unpublished algorithm, in order to design the parallel algorithm presented in Chapter~\ref{ch6}.

negotiate any instances of control or data dependency, and hopefully discover opportunities worthy of exploiting concurrency in order to improve its performance when implemented on a system capable of parallel computation.

\todoBackground{Data dependencies}
Following the pattern set in Section~\ref{ch5sSI}, the parallel algorithm for \Fors{t} also has three main parts. In contrast however, the parallel variant of each of the three serial parts are split further into subroutines, in order to delegate blocks of instructions to separate threads of execution, while protecting critical sections, and avoiding data and control dependencies, so that the threads can be safely executed in parallel.

The following three sections will each focus on producing the parallel variant of one of the three parts of the serial algorithm, by first analyzing the serial algorithm through the lens of parallel programing, then defining the strategy behind the design of the parallel variant, then finally providing the pseudo code definitions to implement that part of the algorithm.

%
%
%
%
\section{Discovering Neighborhoods in Parallel}
\label{ch6sDN}
The sole purpose of Algorithm~\ref{alg:serialBuildNeighborhoods} is to generate a family of sets of neighborhoods $\bN$, by exploring every triangular face in the set, so that subsequent algorithms can recall and iterate over those associations without incurring the computational cost of searching the entire mesh each time. In addition to that goal, the parallel variant of the $\mathit{buildNeighborhoods}$ procedure should also provide a total count of all neighbors in all neighborhoods $\hat{n}$, so that the next two parts of the \fors{t} algorithm can predict and evenly delegate work loads to independent threads of execution.

In Algorithm~\ref{alg:serialBuildNeighborhoods}, line~\ref{sbn2}, the first line of the first function, we encounter a loop over each face $\bt$ in $\bT$. In general, concurrency found in the structure of a loop may be totally exploited by parallel systems, but only in the absence of loop-carried dependence. So, in order to determine if the iterations of the loop may be computed in parallel, an analysis of every operation in the loop block is required.\todoResearch{Amdahl's Law}\todoBackground{loop level parallelism, loop-carried and loop-independent}
\todoBackground{static vs volatile memory}

Figure~\ref{fig:paraBNDataDep} illustrates the data dependencies inherent to each iteration of the loop over the faces $\bt$ in $\bT$, as found from Algorithm~\ref{alg:serialBuildNeighborhoods}. First the triangular face $\bt$ is loaded, as denoted by the teal colored node, from the set $\bT$ in static memory which will never be modified by this, or any other procedure. Also, despite the fact that faces can be adjacent to one another, each face is distinctly defined in the set $\bT$, therefore, each face $\bt$ can be considered independent of each other face. Thus, this first instruction of the loop block is free from any control or data dependencies. The next line in the algorithm is a complex instruction, though, so it must be analyzed in parts.

The first part of the instruction on line~\ref{sbn3}, as denoted by the first three teal colored lines, is to read from face $\bt$ stored in static memory, the indices of the three points $\bp_a$, $\bp_b$, $\bp_c$. Because the face $\bt$ is independent, reading the indices of its three points is also free of dependencies. Next, $\bp_a$ is referenced in order to read the state of $\bN_a$ from volatile memory, as denoted as teal and coral colored lines, respectively. $\bN_a$ is stored in volatile memory, which may potentially be modified by other threads, because it is not known a priori which thread will discover the points which belong to that neighborhood, thus it must be accessible to any and all threads. Next, drawn in sand color, are the two union operations performed in succession on $\bN_a$ with the points $\bp_b$, then $\bp_c$. While these two operations are themselves independent, being performed only on values currently immutable by other procedures, they do rely on having already read the status of $\bN_a$, which indeed does have its own data dependence, hence the sand coloring. Finally illustrated as a double, coral colored arrow, the  the updated set $\bN_a$ is saved back into $\bN$ in volatile memory.

\tikzset{%
	>={Latex[width=2mm,length=2mm]},
	baseNode/.style = {rectangle, rounded corners,
		draw=black, fill=white, thick,
		minimum width=1cm, minimum height=1cm,
		text centered, font=\sffamily, inner sep=.2cm},
	baseLine/.style = {thick},%double},
	tealStyle/.style = {draw=MyTeal, fill=MyLtTeal},
	coralStyle/.style = {draw=MyCoral, fill=MyLtCoral},
	sandStyle/.style = {draw=MySand, fill=MyLtSand},
	%
	indNode/.style = {baseNode, tealStyle},
	indLine/.style = {baseLine, draw=MyTeal},
	depNode/.style = {baseNode, coralStyle},
	depLine/.style = {baseLine, draw=MyCoral},
	mixNode/.style = {baseNode, sandStyle},
	mixLine/.style = {baseLine, draw=MySand},
}
\begin{figure}[ht]
	\begin{tikzpicture}[node distance=0cm]
		\coordinate (center1) at (0cm,0cm);
		\node (sm) [indNode, xshift=-2.25cm] {$\bt$ in static memory};
		\node (vm) [depNode, anchor=west, xshift= 1.25cm] {$\bN$ in volatile memory};

		\coordinate (center2) at (0cm,-2cm);
		\node (pc) [indNode, left of=center2, xshift=-3.50cm] {$\bp_c$};
		\node (pb) [indNode, left of=center2, xshift=-2.25cm] {$\bp_b$};
		\node (pa) [indNode, left of=center2, xshift=-1.00cm] {$\bp_a$};

		\node (Na) [depNode, right of=center2, xshift=1.00cm, yshift=-.5cm] {$\bN_a$};

		\node (union1) at (0cm,-4cm) [mixNode] {$\bN_a \cup \{\bp_b\}$};

		\node (union2) at (0cm,-5.5cm) [mixNode] {$\bN_a \cup \{\bp_c\}$};

		%
		\draw[->, indLine] (sm) -- (pa);
		\draw[->, indLine] (sm) -- (pb);
		\draw[->, indLine] (sm) -- (pc);
		\draw[->, depLine] (vm.220) -- (Na);
		\draw[->, indLine] (pa.east) -- (Na.west);

		\draw[->, indLine] (pb.south) .. controls (-1.75cm,-3.5cm) .. (union1.west);
		\draw[->, mixLine] (Na) -- (union1.north);

		\draw[->, indLine] (pc.south) .. controls (-2cm,-5cm) .. (union2.west);
		\draw[->, mixLine] (union1) -- (union2);
		\draw[->, depLine, double] (union2.east) .. controls (2cm,-5cm) .. (vm);

	\end{tikzpicture}
	{\caption[Data Dependencies in Algorithm for Parallel Build Neighborhoods]{illustrates the data dependencies as found in Algorithm~\ref{alg:serialBuildNeighborhoods}. Operations with data dependencies are in coral color, independent operations are in teal color, and independent operations which reply on operations which have dependencies are in sand color.}\label{fig:paraBNDataDep}}
\end{figure}

Both reading from, and writing to $\bN_a$ in volatile memory constitutes a critical section in the algorithm, which must be accounted for in the design of the parallel variant. Furthermore, both dependencies lie in a single path of dependence, which means that the entire group of operations must be protected by a guarding mechanism as described in Section~\ref{ch2sPPssMS}. Fortunately, because both dependencies revolve around the same set $\bN_a$, a simple mutex per neighborhood will be sufficient. Lines~\ref{sbn4} and~\ref{sbn5} behave similarly to line~\ref{sbn3}, except they concern other neighborhoods, which in turn, need their own mutexes.\todoBackground{path of dependence} Fortunately, by exploiting the fact that the union operation is called exactly twice per each neighborhood for every face which contains its center point, by executing these two operations sequentially within a single mutex, one can mitigate exactly half of the possible collisions in the $\mathit{buildNeighborhoods}$ procedure.

Algorithm~\ref{alg:parallelBuildNeighborhoods} defines the structure and instructions required to implement a parallel variant of the serial procedure, $\mathit{buildNeighborhoods}$, and ensuring correctness by using a set of mutexes. The main function requires only the number of available processors $\rho$, the set of all triangular faces $\bT$, and the cardinality of the set of points $|\bP|$. First, the stride is calculated as the problem size with the unit of work being a single face $\bt$, divided by the number of available processors. Next, a set of mutexes is defined, containing the $|\bP|$ amount of mutexes to be used one each for each neighborhood. \todoReword{remove if no longer setting $\bM$} Then begins the loop to spawn a ``build'' thread for a quarter of the available processors in the system, with the scalar 4 being a result of each build thread's need to spawn 3 additional ``union'' threads. Finally, all the working threads must first synchronize before the family of sets $\bN$ can be realized.

Along with access to the set of triangular face $\bT|$ and set of mutexes $\bM$, each build thread in Algorithm~\ref{alg:parallelBuildNeighborhoods} also requires an index $\Pi$, which it uses along with the stride size $sigma$, to calculate the lower and upper boundaries, $\check{\sigma}$ and $\hat{\sigma}$, for the stride of the problem on which it should compute. Next, it iterates over each face $\bt$ within those stride boundaries, and spawns three union threads; one each for the three points comprising the corners of the current triangular face. Each union thread requires access to not only the family of sets of neighborhoods $\bN$ and the set of mutexes $\bM$, but also the index of the point which it will consider to be central, and two indices of the points which are neighboring it. With those inputs, each union thread then attempts to lock the mutex with the matching index of the central point, blocking until it is able to do so, in order to safely perform the union operation with the currently known neighborhood of the central point, and the set of its two newly discovered neighbors; finally, saving the updated neighborhood to shared memory, then unlocking the mutex.

\begin{algorithm}[ht]
	\DontPrintSemicolon
	\SetCommentSty{small}
	\SetKwFor{For}{for}{:}{}
	\SetKwProg{Func}{Function}{}{}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}

	\Input{the number of available processors $\rho$, \\
		the set of all triangular faces $\bT$, \\
		the cardinality of the set of points $|\bP|$}
	\Output{the family of sets of discovered neighborhoods $\bN$}

	\bigskip
\nl	\Func{parallelBuildNeighborhoods($\rho$, $\bT$, $|\bP|$)}{
\nl		$\sigma \leftarrow |\bT|\mathbin{/}\rho$\tcc*{assuming an integer quotient}
\nl		$\fM \leftarrow \{\mu_1,\,\ldots,\,\mu_{|\bP|}\}$\;
\nl		\For{$\Pi \leftarrow 1$ \KwTo $\rho\mathbin{/}4$}{
\nl			\ProgSty{$\sim$build($\Pi$, $\sigma$, $\fM$, $\bT$)}\;
		}

		\medskip
\nl		\ProgSty{synchronizeThreads()}\;
		\medskip
	}

	\bigskip
\nl	\Func{build($\Pi$, $\sigma$, $\fM$, $\bT$)}{
\nl		$\check{\sigma} \leftarrow (\Pi-1)\,\sigma+1$\tcc*{works through its stride}
\nl		$\hat{\sigma} \leftarrow \Pi\,\sigma$\;
\nl		\For(\tcc*[f]{$\bt = \left \{\bp_a, \bp_b, \bp_c\right \}$}){$\bt \in \{\bt_{\check{\sigma}},\ldots,\,\bt_{\hat{\sigma}}\}$}{
\nl			\ProgSty{$\sim$safeUnion($\fM$, $\bN$, $\bp_a$, $\bp_b$, $\bp_c$)}\;
\nl			\ProgSty{$\sim$safeUnion($\fM$, $\bN$, $\bp_b$, $\bp_a$, $\bp_c$)}\;
\nl			\ProgSty{$\sim$safeUnion($\fM$, $\bN$, $\bp_c$, $\bp_a$, $\bp_b$)}\;
		}
	}

	\bigskip
\nl	\Func{safeUnion($\fM$, $\bN$, $a$, $b$, $c$)}{
\nl		$\ProcSty{lock}(\mu_a)$\;
\nl		$\bN_a \leftarrow \bN_a \cup \{b,\,c\}$\;
\nl		$\ProcSty{unlock}(\mu_a)$\;
	}
	\caption{Parallel algorithm for building the family of sets of all members of each neighborhood discovered in the mesh \label{alg:parallelBuildNeighborhoods}}
\end{algorithm}%
\nomenclature[]{$\rho$}{the number of available processors}%
\nomenclature[]{$\sigma$}{the ``stride'', the size of a block of work intended for a single processor, equal to the problem size divided by the number of available processors}%
\nomenclature[]{$\check{\sigma}$}{the index of the beginning of a stride}%
\nomenclature[]{$\hat{\sigma}$}{the index of the beginning of a stride}%
\nomenclature[]{$\Pi$}{the index representing a single processor}%
\nomenclature[]{$\fM$}{a set of mutexes}%
\nomenclature[]{$\mu_v$}{a specific mutex}%
\nomenclature[]{$\sim process()$}{a process to be run in a new thread}%
\todoAsk{use KwTo or \{...\}?}
\todoAsk{remove the "main" function in each algorithm (except the 5)?}
\todoAsk{define $\bM$, set it as an input, or just use it?}

The motivation for Algorithm~\ref{alg:parallelRecursiveCensusNeighborhoods} is that a total count of all neighbors in all neighborhoods $\hat{n}$, should be provided so that the work load incurred by loops found in the next two parts of \fors{t} algorithm, which iterate over each neighbor in the family of sets $\bN$, can be predicted and evenly delegated to each independent thread of execution. While a running sum of cardinalities could be implemented within Algorithm~\ref{alg:parallelBuildNeighborhoods}, unlike in the serial version, it would very inefficient due to the added overhead of collisions with the guarding mechanism required to protect the total sum.  Algorithm~\ref{alg:parallelRecursiveCensusNeighborhoods} only requires as inputs a pre-built family of sets of neighborhoods $\bN$, and the number of processors available in the system, but is highly parallel and performs at a rate of $O(log_2(n))$,\todoReword{not sure how to word bigOrate} so when using a system with multiple processors, it is a much or efficient alternative to calculating the sum implicitly while building $\bN$.

As Algorithm~\ref{alg:parallelRecursiveCensusNeighborhoods} is a recursive algorithm, it can be described by splitting it into three distinct parts. The first part, ``the termination clause'', is defined as when the cardinality of the current subset of $\bN$ is only two, evaluate the sum of each member $\bN_i$, then return the sum of all the results. Not that this ignores the trivial edge case of families of sets with a very small cardinality of less than three. In all other cases, when the cardinality of the current subset of $\bN$ is greater than two, the second part, ``working towards the termination state'', begins. The second part of this recursive strategy describes summing in parallel, the cardinality or value of each adjacent pair of members in the current subset of $\bN$, then saving the sum of each addition in a new ordered subset of integers $\widetilde{\bN}$, which will have half the cardinality of the current subset. In the third and final part, ``the recursive call'', the function \textit{parallelRecursiveCensusNeighborhoods} calls itself using the new subset $\widetilde{\bN}$ as a parameter instead of the original $\bN$.
\todoReword{big O remark at end is too sudden}
\todoBackground{recursive algorithms, three parts}

\begin{algorithm}[ht]
	\DontPrintSemicolon
	\SetCommentSty{small}
	\SetKwFor{For}{for}{:}{}
	\SetKwProg{Func}{Function}{}{}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}

	\Input{the number of available processors $\rho$, \\
		the family of sets of neighborhoods $\bN$}
	\Output{the count of all neighbors in all neighborhoods $\hat{n}$}

	\bigskip
\nl	\Func{parallelRecursiveCensusNeighborhoods($\rho$, $\bN$)}{
\nl		\eIf{$|\bN| \leq 2$}{
\nl			$\hat{n} \leftarrow \sum_{i=1}^{|\bN|}\bN_i$\;%\underset{i=1}{\overset{|\bN|}{\sum}}|\bN_i|$\;
		}{%Else
\nl			$\sigma \leftarrow |\bN|\mathbin{/}(2\,\rho)$\;
\nl			\For{$\Pi \in \{1,\ldots,\rho\}$}{
\nl				\ProgSty{$\sim$parallelSum($\Pi$, $\sigma$, $\bN$)}\;
			}
			\medskip
\nl			\ProgSty{synchronizeThreads()}\;
			\medskip
\nl			\ProgSty{parallelRecursiveCensusNeighborhoods($\rho$, $\widetilde{\bN}$)}\;
		}
	}

	\bigskip
\nl	\Func{parallelSum($\Pi$, $\sigma$, $\bN$)}{
\nl		$\check{\sigma} \leftarrow (\Pi-1)\,\sigma+1$\;
\nl		$\hat{\sigma} \leftarrow 2\,\Pi\,\sigma$\;
\nl		\For{$v \in \{\check{\sigma},\,\check{\sigma}\sps{}2,\,\check{\sigma}\sps{}4,\ldots,\,\hat{\sigma}\}$}{
\nl			\eIf(\tcc*[f]{only occurs in first call}){$\bN$ is a family of sets}{\label{algPRCNbNdistinction}
\nl				$\widetilde{\bN_v} \leftarrow |\bN_v| + |\bN_{\sxpx{v}{1}}|$\;
			}{%else
\nl				$\widetilde{\bN_v} \leftarrow \bN_v + \bN_{\sxpx{v}{1}}$\;
			}
		}
	}
	\caption{Parallel algorithm for recursively counting a census of all neighbors in all neighborhoods \label{alg:parallelRecursiveCensusNeighborhoods}}
\end{algorithm}%
\nomenclature[]{$\hat{n}$}{a census, the count of all neighbors in all neighborhoods}%
\nomenclature[]{$\widetilde{\bN}$}{a subset of $\bN$ used temporarily in Algorithm~\ref{alg:parallelRecursiveCensusNeighborhoods}}%
\todoReword{mention that non-existent neighborhoods should be treated as having a cardinality of 0}

It is important to notice in line~\ref{algPRCNbNdistinction}, the distinction between the family of sets $\bN$ in the first call of \textit{parallelRecursiveCensusNeighborhoods}, with that of the set of integers accumulating in the subset $\widetilde{\bN}$ in subsequent calls.

%
%
%
%
\section{Calculating Edge Lengths in Parallel}
\label{ch6sCEL}
Next, we examine the serial Algorithm~\ref{alg:serialCalculateEdgeLengths}, which has the goal of building a set of pre-calculated edge lengths $\bE$, as well as determining the global minimum edge length $\gelm$; both essential parameters of  Algorithm~\ref{alg:serialConvolveFilter}. Already in line~\ref{scel1}, we encounter a loop over each point $\bp_v$ in $\bP$. In order to determine if the iterations of the loop may instead be computed in parallel, an analysis of every operation in the loop block is required, which in this case, includes lines~\ref{scel2}-~\ref{scel5}. The first internal line,~\ref{scel2}, starts another loop over each point $\bp_i$ in $\bN_v$. \todoReword{why important to know ?} As we have seen very clearly in Figure~\ref{fig:neighborhoods}, the cardinality of each individual neighborhood can not be predicted for irregular, triangle meshes, like those typical of acquired \tdd{}. However, that becomes less important because it is possible to know the total count of neighbors of in all neighborhoods $\hat{n}$, by counting the size of each neighborhood a posteriori, as is done in Algorithm~\ref{alg:parallelRecursiveCensusNeighborhoods}. With $\hat{n}$, we may unroll this pair of loops and instruct a calculable number of threads to process the loop block in parallel. If the average number of neighbors per point is six\todoResearch{average number of neighbors}, then the number of edge lengths to be calculated and stored will be six times as large as the cardinality of $\bP$.

Line~\ref{scel3} of Algorithm~\ref{alg:serialCalculateEdgeLengths} is the $\ellstar$ operation, the most costly operation performed by \fors{t}, due to use of the $\sqrt{(\cdot)}$ operation. Therefore, unlike with the union operation in Algorithm~\ref{alg:parallelBuildNeighborhoods}, it is of paramount importantance that we avoid any unnecessary duplication of the $\ellstar$ operation. In Algorithm~\ref{alg:serialBuildNeighborhoods}, each pair of adjacent points are represented twice, being indexed once from both directions, and while calculating the length both times is exactly what we want to avoid, it is a design choice related to Section\todoReference{memory vs speed}\todoBackground{memory vs speed} whether to store the length twice. The benefit of storing the set of adjacent points in this way, it that it creates an implicit reverse lookup-table which is well documented\todoCitation{multiples, reverse lookup table} for increasing the speed of computations, and further simplifies the complexity of indexing the values, conversely to save memory, one could store the value only once by implementing the control structures for detecting if an edge length has already been saved \todoReference{more details about this method in future work}, then when retrieving the values, one could search for the edge length required at the cost of compute time. We have user-defined to detail the first, speedier method.

Algorithm~\ref{alg:parallelCalculateEdgeLengths} is the parallel algorithm for calculating all the edge lengths between each pair of adjacent points in the mesh. It requires the knowledge of and access to the number of available processors in the system $\rho$, the set of all points $\bP$, the family of sets of discovered neighborhoods $\bN$, the average size of every neighborhood in $\bar{n}$, and the count of all neighbors in all neighborhoods $\hat{n}$, and produces as an output, the set of pre-calculated edge lengths $\bE$, and the global minimum edge length $\gelm$; both instrumental to the calculation of \Fors{t}. This algorithm has been split further into three subroutines in order to maximize efficiency by facilitating the balance of work loaded in parallel onto each processor.

\begin{algorithm}[ht]
	\DontPrintSemicolon
	\SetCommentSty{small}
	\SetKwFor{For}{for}{:}{}
	\SetKwProg{Func}{Function}{}{}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}

	\Input{the number of available processors $\rho$, \\
		the set of all points $\bP$, \\
		the family of sets of discovered neighborhoods $\bN$, \\
		the average size of every neighborhood in $\bar{n}$, \\
		the count of all neighbors in all neighborhoods $\hat{n}$}
	\Output{the set of pre-calculated edge lengths $\bE$, \\
		the global minimum edge length $\gelm$}

	\bigskip
\nl	\Func{parallelCalculateEdgeLengths($\rho$, $\bP$,\,$\bN$,\,$\hat{n}$)}{
\nl		$\sigma \leftarrow \hat{n}\mathbin{/}\rho$\label{algPCELellstar}\;
\nl		\For{$\Pi \in \{1,\,\ldots,\,\left \lceil\rho/\bar{n}\right\rceil\}$}{
\nl			\ProgSty{$\sim$calculateLengths($\Pi$, $\sigma$, $\mu$, $\bP$,\,$\bN$)}\;
		}

		\medskip
\nl		\ProgSty{synchronizeThreads()}\;
		\medskip
	}

	\bigskip
\nl	\Func{calculateLengths($\Pi$, $\sigma$, $\mu$, $\bP$,\,$\bN$)}{
\nl		$\check{\sigma} \leftarrow (\Pi-1)\,\sigma+1$\;
\nl		$\hat{\sigma} \leftarrow \Pi\,\sigma$\;
\nl		\For{$\bp_v \in \{\bp_{\check{\sigma}},\ldots,\,\bp_{\hat{\sigma}}\}$}{
\nl			\For{$\bp_i \in \bN_v$}{
\nl				\ProgSty{$\sim$safeEdgeLengthCalculation($\mu$, $\bE$, $\gelm$, $\bp_v$, $\bp_i$)}\;
			}
		}
	}

	\bigskip
\nl	\Func{safeEdgeLengthCalculation($\mu$, $\bE$, $\gelm$, $\bp_v$, $\bp_i$)}{
\nl		$\bE_{\sv{i}} \leftarrow |\bp_i - \bp_v|$\tcc*[r]{$\ellstar$, Eq:~\ref{eq:localMinimumEdgeLength}}
\nl		\If(\tcc*[f]{heuristic only\footnotemark}){$\bE_{\sv{i}} < \gelm$}{\label{algPCELhcs}
\nl			$\ProcSty{lock}(\mu)$\;
\nl			$\gelm \leftarrow \min\left \{\gelm,\,\bE_{\sv{i}}\right \}$\label{algPCELgelm}\tcc*[r]{Eq:~\ref{eq:globalMinimumEdgeLength}}
\nl			$\ProcSty{unlock}(\mu)$\;
		}
	}
	\caption{Parallel algorithm for calculating all the edge lengths between each pair of adjacent points in the mesh\label{alg:parallelCalculateEdgeLengths}}
\end{algorithm}
\footnotetext{While it is true that we are attempting to avoid any unnecessary edge length calculations or mutex locks, the hidden message in this line is honestly just a happy accident.}
\todoBackground{lock/unlock/mutex}
\todoReword{$\bE_{\sv{i}}$ is a unique address, so no mutex required}
\todoBackground{future work, can calculate average neighborhood size in alg.1 or 4}

The initial function, \textit{parallelCalculateEdgeLengths}, requires all of the inputs listed in the previous paragraph, then calculates the stride with the problem size equating to the count of all neighbors in all neighborhoods $\hat{n}$. Next, the set of mutexes is prepared, and the loop iterating over a portion of the count of processors is encountered, calling the \textit{calculateLengths} function in each iteration. Because each thread will be expected to spawn an additional thread for each pair of adjacent points found in every neighborhood in its designated stride of the work load, the greater portion of processors, scaled to the average neighborhood size of the mesh $\bar{n}$, is held in reserve for those spawned threads to be able to run in parallel. Finally, all the working threads must synchronized before the set of pre-calculated edge lengths $\bE$ before the final value of $\gelm$ may be used.

The \textit{calculateLengths} function requires an index $\Pi$, which it uses along with the stride size $sigma$, to calculate the lower and upper boundaries, $\check{\sigma}$ and $\hat{\sigma}$, for the stride of the total problem which it should compute. Next, the function iterates over each point $\bp_v$ within its stride boundaries, as well as each neighbor $\bp_i$ in the current center point's neighborhood $\bN_v$, spawning new threads to execute instances of the \textit{safeEdgeLengthCalculation} procedure in each iteration.

The function, \textit{safeEdgeLengthCalculation}, requires access to the set of edge lengths $\bE$ being collaboratively populated by each other instance of itself, as well as the single mutex $mu$ used to guard the final reading of and writing to the shared value of $\gelm$. Naturally, also required are the coordinates of the two points, between which the distance is being calculated. In line~\ref{algPCELellstar}, the first line of the function, the L2-norm of the difference between the two points is already calculated; this is the $\ellstar$ operation. There are two race conditions in line~\ref{algPCELgelm} which must be avoided, however it would be incredibly inefficient to have all $(\bar{n}-\rho)\mathbin{/}\bar{n}$ threads attempt to lock the same, single mutex $\mu$. The solution is the heuristic conditional statement in line~\ref{algPCELhcs}, testing the worthiness of incurring the cost of attempting to lock the shared mutex guarding the reads and writes to $\gelm$. We call this conditional statement heuristic, in order to call attention to the fact that it may give an inaccurate result due to race conditions between the threads operating in parallel. \todoResearch{just how much time can be saved with this heuristic?}

In the next section, we will see how the pre-calculations of Algorithms~\ref{alg:parallelBuildNeighborhoods}, ~\ref{alg:parallelRecursiveCensusNeighborhoods}, and ~\ref{alg:parallelCalculateEdgeLengths} contribute to the speed and scalability of the main procedure, alongside the modifications to exploit the concurrency inherent to the filter.

%
%
%
%
\section{Convolving the Filter in Parallel}
\label{ch6sCF}
Finally, we examine the serial Algorithm~\ref{alg:serialConvolveFilter}, which includes the principle loop to convolve \Fors{t}. As a prerequisite, one must have already run the previous three parallel algorithms, as detailed in Sections~\ref{ch5sPAssBN} and~\ref{ch5sPAssCEL}, in order to generate the input parameters which will be used in the parallel variant of this algorithm.

In the first line of the serial Algorithm~\ref{alg:serialConvolveFilter}, we encounter a loop which runs for a user-defined number of convolutions. In order to determine if one can parallelize the entire loop, once must closely analyze each operation within the loop block; searching for loop-carried control or data dependencies. In line~\ref{algSCFlastLine}, the last line of the loop block, the set of function values $\bF$ is replaced by the newly computed set of weighted mean function values $\bF'$, before starting the next iteration. This models exactly the definition of a loop-carried data dependency, therefore it would be impossible to calculate this loop entirely\footnote{What may be possible, would be to pipeline calculations for selected points between iterations, since calculation only actually require the new function values of its neighbors, not the entire set. However, given the effort required to compute a single iteration of \fors{t} on typical acquired \tdd{}, it is unlikely that any additional speedup will be realized using such a technique, computing with the GPGPUs commercially available today.} in parallel.

In the second and third lines of Algorithm~\ref{alg:serialConvolveFilter}, we encounter two more loops iterating over each point in $\bP$, and then each neighborhood $\bN_v$ associated with those points. We have seen this pattern before;\todoReword{talk about pattern when we see it first}together, these two loops iterate over the family of sets of neighborhoods $\bN$, and indeed can be unrolled and computed in parallel barring any occurrences of dependicies within the loop block. Lines~\ref{algSCFalpha} -~\ref{algSCFcheckf} only  generate new terms which are unique to the current iteration, using only original values which are not changed by any other thread, therefore are all prime candidates for exploiting concurrency in the loop. Line~\ref{algSCFjloop} starts a new loop over just two values, and while it would be possible to compute both halves of the values the loop in parallel, at this point in the computation, we would expect all processors to already be in use calculating the other iterations, so dedicating the resources to spawn new threads here would likely net a loss in efficiency, so we will simply unroll the loop and compute the four new terms in serial, per thread.

Lines~\ref{algSCFtildef} and~\ref{algSCFtildea} both accumulate values in shared memory which will be accessed in parallel, on average $\bar{n}$ times, setting up race conditions and threatening the correctness of the algorithm. Because of the limited number of possible collisions, we will treat these two lines as a single critical section, protecting each pair with a shared mutex as we did for the paired union operations in Algorithm~\ref{parallelBuildNeighborhoods}. However, in contrast, these mutexes will have a limited scope, not being shared globally, but only shared within a local neighborhood.

In line~\ref{algSCFfprimev}, the values $\tilde{f_v}$ and $\tilde{A_v}$, having been accumulated for each neighbor in $\bN_V$, are used to calculate the weighted mean function value for the entire geodesic disc $f'_v$, but before their values are read, all threads working on that neighborhood must be synchronized to ensure the correct values are used. As thread synchronization is quite expensive to computation time\todoReference{syncing is expensive}, it is imperative that only those threads working on the neighborhood are synchronized, and not all threads, as is necessary in most other parts of this algorithm, including between the next two lines.

In lines~\ref{algSCF2ndlastline} and~\ref{algSCFlastline}, all the values of $f'_v$ from each neighborhood in $\bN$ are collected into the new set $\bF'$,  then either returned as the final result of the filter's convolutions, or used as the scalar field of function values in the next iteration of the filter. Saving the values to the new set may be done in parallel, because each value will remain unique in the set, however, before the new set may be used in either way all threads of execution must have time to synchronize in order to ensure that only the correct values are used.

Now that we have analyzed, the serial Algorithm~\ref{alg:serialConvolveFilter}, and discovered opportunities where exploiting the loop-level concurrency will benefit the efficiency of \fors{t}, let us not hesitate to create the parallel variant.

Algorith~\ref{alg:parallelConvolveFilter}, is split into three functions. The first function \textit{parallelConvolveFilter} requires several inputs which include: the number of available processors $\rho$, the set of all points $\bP$, the family of sets of discovered neighborhoods $\bN$, the count of all neighbors in all neighborhoods $\hat{n}$, the set of pre-calculated edge lengths $\bE$, the global minimum edge length $\gelm$, the set of function values $\bF$, and the user-defined number of convolutions $\tau$.

In the first line, line~\ref{algPCFtauloop}, begins the loop iterating for the user-defined number of times $\tau$. Initially, the stride size $\sigma$ is calculated with the work load computed as one for each point defining a geodesic disc $\bO$, which is equal to the cardinality of points in the mesh $|\bP|$, plus one for each neighboring point in every neighborhood $\hat{n}$. Next, another loop is begun, which iterates over a portion of the available processors in the system, as we did in Algorithm~\ref{alg:parallelCalculateEdgeLengths}, reserving resources for the rest of the threads that must be spawned in order to compute in parallel, the weighted mean function value over an entire neighborhood $f'_v$. In each iteration of this loop, a new thread is spawned to execute the function \textit{safeAccumulateGeoDiscMean}, passing to it as parameters: a processor's index $\Pi$, the stride length $\sigma$, the new scalar field which is to be populated with calculated weighted mean function values from each neighborhood $\bF'$, and the other general information defining and describing the mesh, $\bP$, $\bN$, $\bE$, $\gelm$, $\bF$. At the conclusion of the loop block, all threads are synchronized before either the new scalar field of weighted mean function values $\bF'$ are used as the function values upon which the next convolution of the filter is based, or the convolutions end and $\bF'$ is returned as the final result.

In line~\ref{algPCFsagdmCall}, the function \textit{safeAccumulateGeoDiscMean} is called for each processor, not reserved for sector-wise computations. That function begins by using its designated index $\Pi$, to compute the lower and upper boundaries, $\check{\sigma}$ and $\hat{\sigma}$, for the stride of the problem on which it should compute. Next, the function iterates over each point $\bp_v$ within its stride boundaries, creating a new set of mutexes $\bM$, containing one $\mu$ per neighbor in currently neighborhood $\bN_v$, before beginning another loop. This inner loop iterates over each neighbor $\bp_i$ in the neighborhood $\bN_v$, and spawns new threads to execute instances of the \textit{safeAccumulateSectorMean} subroutine. After the loop of $\bN_v$ finishes, the threads which were used to calculate the weighted mean function values for each circle sector are synchronized, before their values are used in the calculation of $f'_v$, the weighted mean function value at point $\bp_v$. While $\bF'$ is a set shared among all the threads, the assignment of $f'_v$ does not require a mutex to guard against race conditions, because during any given convolution of the filter, only one single thread will ever compute and attempt to store the value of $f'_v$ at the location $\bF'_v$.

In line~\ref{algPCFsasmCall}, a new thread is spawned to execute an instance of the function \textit{safeAccumulateSectorMean} for each sector in the current neighborhood. In the first nine lines of \textit{safeAccumulateSectorMean} culminate in the computation of $\check{f}$, the weighted mean function value, for the current circle sector being processed. Because the goal is to accumulate these new values, as well as the area for each sector, in order to be used in the calculation of the weighted mean over the entire geodesic disc, we chose to guard the shared memory values $\tilde{f}_v$ and $\tilde{A}_v$ using mutexes, risking on average a small number of collisions, only $\bar{n}$ per neighborhood. An alternative solution, which we choose not to implement here because it would cost $2\cdot\hat{n}$ more floating point values worth of memory, is to save each intermediate value separately, then perform the averaging calculation on those values after having synchronized all the threads.

\begin{algorithm}[ht]
	\DontPrintSemicolon
	\SetCommentSty{small}
	\SetKwFor{For}{for}{:}{}
	\SetKwProg{Func}{Function}{}{}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}

	\Input{the number of available processors $\rho$, \\
		the set of all points $\bP$, \\
		the family of sets of discovered neighborhoods $\bN$, \\
		the count of all neighbors in all neighborhoods $\hat{n}$, \\
		the average size of every neighborhood in $\bar{n}$, \\
		the set of pre-calculated edge lengths $\bE$, \\
		the global minimum edge length $\gelm$, \\
		the set of function values $\bF$, \\
		the user-defined number of convolutions $\tau$}
	\Output{the set of one-ring weighted mean function values $\bF'$}

	\bigskip
	\linespread{1}\selectfont
\nl	\Func{parallelConvolveFilter($\rho$, $\bP$, $\bN$, $\hat{n}$, $\bar{n}$, $\bE$, $\gelm$, $\bF$, $\tau$)}{
\nl		\For{$t\leftarrow 1\;\KwTo\;\tau$}{\label{algPCFtauloop}
\nl			$\sigma \leftarrow (|\bP|+\hat{n})\mathbin{/}\rho$\;
\nl			\For{$\Pi \in \{1,\ldots,\left \lceil\rho/\bar{n}\right\rceil\}$}{
\nl				\ProgSty{$\sim$safeAccumulatesGeoDiscMean($\Pi$, $\sigma$, $\bP$, $\bN$, $\bE$, $\gelm$, $\bF$, $\bF'$)}\;\label{algPCFsagdmCall}
			}
\nl			\ProgSty{synchronizeThreads()}\;
			\medskip

\nl 		\If(\tcc*[f]{smooth newest values every iteration}){$t < \tau$}{$\bF \leftarrow \bF'$}
		}
\nl 	\KwRet $\bF'$\;
	}

	\bigskip
\nl	\Func{safeAccumulatesGeoDiscMean($\Pi$, $\sigma$, $\bP$, $\bN$, $\bE$, $\gelm$, $\bF$, $\bF'$)}{
\nl		$\check{\sigma} \leftarrow (\Pi-1)\,\sigma+1$\;
\nl		$\hat{\sigma} \leftarrow \Pi\,\sigma$\;
\nl		\For{$\bp_v \in \{\bp_{\check{\sigma}},\ldots,\,\bp_{\hat{\sigma}}\}$}{
\nl			$\fM \leftarrow \left \{\mu_1,\dots,\mu_{|\bN_v|}\right \}$\;
\nl			\For{$\bp_i \in \bN_v$}{
				\smallskip
\nl				\ProgSty{$\sim$safeAccumulateSectorMean($\fM$, $\bE$, $\gelm$, $\bp_v$, $\bp_i$, $\bF$, $\tilde{f}_v$, $\tilde{A}_v$)}\;\label{algPCFsasmCall}
			}
\nl			\ProgSty{synchronizeThreads(\{$\check{\sigma}$, \ldots, $\hat{\sigma}$\})}\;

			\medskip
\nl			$\bF'_v \leftarrow \tilde{f}_v\mathbin{/}\tilde{A}_v$\tcc*[r]{as $f'_v$, Eq:~\ref{eq:meanFuncValAtPv}}
		}
	}

	\bigskip
\nl	\Func{safeAccumulateSectorMean($\fM$, $\bE$, $\gelm$, $\bp_v$, $\bp_i$, $\bF$, $\tilde{f}_v$, $\tilde{A}_v$)}{
		\linespread{1.5}\selectfont
\nl		$\kern-0.5pt\alpha \leftarrow cos^{-1}$
		\begin{Large}
			$\kern-6pt\left (\frac{\bE_c^2\,+\,\bE_b^2\,-\,\bE_a^2}{2\,\cdot\,\bE_c\,\cdot\,\bE_b}\right )$\tcc*[r]{Eq:~\ref{eq:alphaFromEdgeLengths}}
		\end{Large}
		\linespread{1.2}\selectfont
\nl		$\kern0.00pt\beta \leftarrow (\pi - \alpha)\mathbin{/}2$\tcc*[r]{Eq:~\ref{eq:betaFromHalfAlpha}}
\nl		$\kern-1.5ptA \leftarrow \Big(\gelm\,\Big)^2\kern-4pt\cdot\alpha\mathbin{/}2$\tcc*[r]{Eq:~\ref{eq:circularSectorArea}}
\nl		$\kern1.00pt\check{\ell} \leftarrow \big(4\cdot\gelm\cdot\sin(\alpha\mathbin{/}2)\big)\mathbin{/}3\,\alpha$\tcc*[r]{Eq:~\ref{eq:distToCoG}}
\nl		$\kern1.00pt\zeta \leftarrow \gelm\mathbin{/}\sin(\beta)$\tcc*[r]{Eq:~\ref{eq:zeta}}
\nl		\For{$j \in {1,2}$}{
\nl			$\tilde{\ell}_j \leftarrow \zeta\mathbin{/}\bE_j$\tcc*[r]{Eq:~\ref{eq:distanceIForInterpolationalgSCFlastLine},~\ref{eq:distanceIp1ForInterpolation}}
\nl			$f'_j \leftarrow f_0\cdot(1 - \tilde{\ell}_j) + f_j\cdot\tilde{\ell}_j$\tcc*[r]{Eq:~\ref{eq:interpolatedFi},~\ref{eq:interpolatedFip1}}
		}
\nl		$\check{f} \leftarrow f_0\cdot(1 - \check{\ell}) + \big((f'_1 + f'_2)\cdot\check{\ell}\big)\mathbin{/}2$\tcc*[r]{Eq:~\ref{eq:weightedMeanAtCoGatSector}}
		\linespread{1.0}\selectfont
\nl		$\ProcSty{lock}(\mu)$\;
\nl		$\kern2.0pt\tilde{f}_v \leftarrow \tilde{f}_v + A\cdot\check{f}$\tcc*[r]{Eq:~\ref{eq:meanFuncValAtPv}}
\nl		$\kern0.0pt\tilde{A}_v \leftarrow \tilde{A}_v + A$\tcc*[r]{Eq:~\ref{eq:meanFuncValAtPv}}
\nl		$\ProcSty{lock}(\mu)$\;
	}

	\caption{Parallel algorithm for convolving \Fors{t}\label{alg:parallelConvolveFilter}}
\end{algorithm}%
%\todoReword{ensure counter is real value}
\todoReword{make sure sigmas include control htereads too}
\todoReword{Alg3,6 output}

The final result is that the weighted mean function values $\tilde{f'}_v$ and areas $\tilde{A}_v$ of each circle sector are calculated in parallel, then those values are averaged, also in parallel, over the total area of the geodesic disc centered at each $\bp_v$, and collected in the set $\bF'$, completing a single convolution of \Fors{t}.

%
%
%
%
\section{Summary}
\ldots
