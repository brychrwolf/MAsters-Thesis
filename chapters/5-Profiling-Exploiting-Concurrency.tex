\chapter{Profiling \& Exploiting Concurrency}
\label{ch5}
In the previous chapter we presented an improved version of \Forf{t} as it is currently implemented within the GigaMesh framework, and while it has improved accuracy in regards to the weights, it is still yet entirely serial in design, so that unfortunately, its performance suffers greatly under the complexity of modern mesh sizes, which with the current high resolution scanners in use, can grow to be MESH\_SIZES\todoResearch{mesh sizes}. We will now explore this as-yet-unpublished algorithm in order to discover any instances data dependency, and opportunities worthy of exploiting concurrency in order to improve its performance when implemented on a system capable of parallel computation.

%
%
%
%
\section{Serial Algorithms}
\label{ch5sSI}
In this section, we now combine all of the equations from the previous chapter,~\ref{eq:localMinimumEdgeLength} through~\ref{eq:meanFuncValAtPv}, into a three-part algorithm using mathematical pseudo-code, with the goal of facilitating the implementation of the improved version of \forf{t}. However, before one is able to begin convolving the filter, the computations require that the one-ring neighborhoods already be known. Then during the convolutions, the filter uses all edge lengths at least once per iteration, and twice for non-border edge lengths, which are shared between adjacent neighborhoods. Therefore, it is beneficial to split the algorithm into three distinct parts, then save the results of the first two parts to be used during the iterative convolutions of the third part, with the result being a massive increase in efficiency by greatly reducing the number of operations-per-iteration required.
\todoBackground{add convolution and convolve to background}
\todoBackground{memory vs speed cost compromise}

%
%
\subsection{Discovering Neighborhoods}
\label{ch5sSIssDN}
Initially, one must discover all the points $\bp_i$ which comprise each neighborhood $\bN_v$ in the entire mesh $\bM$, which is the purpose of Algorithm~\ref{alg:serialBuildNeighborhoods}. Although building this family of sets outside of the principle loop adds an additional $2\cdot |\bT|^3$ operations in total, doing so enables Alogrithm~\ref{alg:serialCalculateEdgeLengths} to exploit this family of sets to vastly reduce its complexity from $|\bP|^{|\bT|}$ to approximately\footnote{depending on the average size of all neighborhoods, assumed here to be about 6}\todoResearch{find good average for average neighborhood size} $|\bP|^6$. Also, as we will see in Algorithm~\ref{alg:serialConvolveFilter}, when $\tau$ is the chosen number of iterations to perform, the complexity of the main procedure can be significantly reduced\todoReword{can be significantly reduced} to only $\tau^{|\bP|^{6*3}}$, down from the $\tau^{|\bP|^{(|\bF|*3)}}$ that would have been necessary had the neighborhoods not already been discovered and the procedure been otherwise required to discover the members of $\bN_v$ in each iteration.%
\nomenclature[na]{$\tau$}{the chosen number of iterations to perform}%
\todoBackground{family of sets}
\todoBackground{complexity, big O notation}

\begin{algorithm}[h]
	\DontPrintSemicolon
	\SetCommentSty{small}
	\SetKwFor{For}{for}{:}{}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}

	\Input{the set of all triangular faces $\bT$}
	\Output{the family of sets of discovered neighborhoods $\bN$}

	\bigskip
	\FuncSty{serialBuildNeighborhoods}\FuncArgSty{($\bT$)}\;
\nl	\For(\tcc*[f]{$\bt = \left \{\bp_a, \bp_b, \bp_c\right \}$}){$\bt \in \bT$}{\label{sbn1}
		\linespread{1.5}\selectfont
\nl		$\bN_a \leftarrow \bN_a \cup \left \{ \bp_b, \bp_c \right \}$\label{sbn2}\;
\nl		$\bN_b \leftarrow \bN_b \cup \left \{ \bp_a, \bp_c \right \}$\label{sbn3}\;
\nl		$\bN_c \leftarrow \bN_c \cup \left \{ \bp_a, \bp_b \right \}$\label{sbn4}\;
	}
	\caption{Serial algorithm for building the family of sets of all discovered members of each neighborhood in the mesh\label{alg:serialBuildNeighborhoods}}
\end{algorithm}%

%
%
\subsection{Calculating Edge Lengths}
\label{ch5sSIssCEL}
In the next step, Algorithm~\ref{alg:serialCalculateEdgeLengths} calculates all the distinct edge lengths $\ell_{\sv{i}}$ which are present in the mesh $\bM$, as well as the global minimum edge length $\gelm$.

As shown in Equation~\ref{eq:defineEdgeLengthPoint}, the calculation of an edge's length requires taking the L2-norm of the difference between two points, which involves using the square root operation. In modern software, the square root operation is performed by computing using Newton's method\todoCitation{software uses Newton's iteration for sqrt}, which is essentially multiple iterations of the so-called, recurrence equation\todoCitation{Newton's iteration uses recurrence equation}. This procedure is otherwise known as ``Newton's Iteration.'' ~\cite{Weisstein19b}

The impact for \forf{t} is that the computation of a square root typically\todoReword{add footnote with desc and citation}\todoCitation{how slow is Newton's iteration compared to others} takes many more compute cycles than other any other binary or unary operation, thus taking more time to complete overall. In fact, because of the slowness of the square root operation, computing the L2-norm in order to calculate an edge's length is empirically the most costly operation performed by the filter\todoResearch{qualify, do experiment to prove how slow sqrt is}. Therefore, it is imperative that we pay special attention to avoid unnecessary calls to calculate an edge's length. For that reason, we define the symbol $\ellstar$ to represent the calculation of an edge's length using "Newton's Iteration", so as to draw focus to its importance while designing an efficient implementation of \forf{t}.

So, while building this set outside of the principle loop requires ${\ellstar}$ to be calculated about $|\bP|^6$ times\footnote{depending on the average size of all neighborhoods, assumed here to be about 6}, it would be otherwise impossible to calculate the globally shortest edge length $\gelm$. However, if we also store the results of each edge length in the set $\bE$, doing so enables the main procedure to completely exclude all $\ellstar$ operations from the principle loop, and as we will see in Algorithm~\ref{alg:serialConvolveFilter}, that reduces the total count of $\ellstar$ operations which must be performed from $(2\,\ell_{\sv{i}}^{\,border} + 4\,\ell_{\sv{i}}^{\,non-border})\cdot\tau^{|\bP|}$, down to only the initial $1\cdot |\bP|^6$, becoming completely independent of $\tau$ and significantly more efficient overall than had the procedure been required to calculate an edge length each time it was used during computation.%
\nomenclature[oa]{$\ellstar$}{the procedure of calculating an edge's length using ``Newton's Iteration'', the most costly operation in the Fast One-Ring filter, due to use of $\sqrt{(\cdot)}$}%
\nomenclature[ob]{$\bE$}{a set of pre-calculated edge-lengths}%
\todoBackground{memory vs speed cost compromise}
\todoBackground{border vs non-border edge lengths}
\todoReword{three big paragraphs in a row is not pretty}

\begin{algorithm}[h]
	\DontPrintSemicolon
	\SetCommentSty{small}
	\SetKwFor{For}{for}{:}{}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}

	\Input{the set of all points $\bP$, \\
		the family of sets of discovered neighborhoods $\bN$}
	\Output{the set of pre-calculated edge-lengths $\bE$, \\
		the globally shortest edge length $\gelm$}

	\bigskip
	\FuncSty{calculateEdgeLengths}\FuncArgSty{($\bP$,\,$\bN$)}\;
\nl	\For{$\bp_v \in \bP$}{\label{scel1}
\nl		\For{$\bp_i \in \bN_v$}{\label{scel2}
			\linespread{1.5}\selectfont
\nl			$\ell_{\sv{i}} \leftarrow \|\bp_i - \bp_v\|_2$\tcc*[r]{Eq:~\ref{eq:localMinimumEdgeLength}}\label{scel3}
\nl			$\bE_{\sv{i}} \leftarrow \ell_{\sv{i}}$\tcc*[r]{\footnotemark}\label{scel4}
\nl			$\gelm \leftarrow \min\left \{\gelm,\,\ell_{\sv{i}}\right \}$\tcc*[r]{Eq:~\ref{eq:globalMinimumEdgeLength}}\label{scel5}
		}
	}
	\caption{Serial algorithm for calculating all the edge lengths between each pair of adjacent points in the mesh\label{alg:serialCalculateEdgeLengths}}
\end{algorithm}
\footnotetext{While it is true that we are attempting to avoid any unnecessary edge length calculations, the hidden message in this line is honestly just a happy accident.}

%
%
\subsection{Convolving the Filter}
\label{ch5sSIssCF}
In the third and final part, we present Algorithm~\ref{alg:serialConvolveFilter}, which illustrates the remaining steps required to convolve \Forf{t}. After having completed the pre-calculations of Algorithms~\ref{alg:serialBuildNeighborhoods} and~\ref{alg:serialCalculateEdgeLengths}, each convolution is performed
for a user-defined number of iterations $\tau$,
by iterating over over the entire mesh $\bM$,
each point $\bp_j$,
which is a corner defining each half of the circle sector $\bs_i$,
which itself is defined by the point $\bp_i$,
that when together, comprise each neighborhood $\bN_v$,
which is determined by each point $\bp_v$.

\begin{algorithm}[h]
	\DontPrintSemicolon
	\SetCommentSty{small}
	\SetKwFor{For}{for}{:}{}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}

	\Input{the set of all points $\bP$, \\
		the family of sets of discovered neighborhoods $\bN$, \\
		the chosen number of iterations $\tau$, \\
		the set of pre-calculated edge-lengths $\bE$, \\
		the globally shortest edge length $\gelm$}
	\Output{new function values}

	\bigskip
	\linespread{1}\selectfont
	\FuncSty{convolveFilter}\FuncArgSty{($\bP$,\,$\bN$,\,$\bE$,\,$\tau$)}\;
	\nl\For{$\tau\leftarrow 1\;\KwTo\;\#iterations$}{
	\nl	\For{$\bp_v \in \bP$}{
	\nl		\For{$\bp_i \in \bN_v$}{
				\linespread{1.5}\selectfont
	\nl			$\kern-0.5pt\alpha \leftarrow cos^{-1}$
					\begin{Large}
						$\kern-6pt\left (\frac{\bE_c^2\,+\,\bE_b^2\,-\,\bE_a^2}{2\,\cdot\,\bE_c\,\cdot\,\bE_b}\right )$\tcc*[r]{Eq:~\ref{eq:alphaFromEdgeLengths}}
					\end{Large}
	\nl			$\kern0.00pt\beta \leftarrow (\pi - \alpha)\mathbin{/}2$\tcc*[r]{Eq:~\ref{eq:betaFromHalfAlpha}}
	\nl			$\kern-1.5ptA \leftarrow \Big(\gelm\,\Big)^2\kern-4pt\cdot\alpha\mathbin{/}2$\tcc*[r]{Eq:~\ref{eq:circularSectorArea}}
	\nl			$\kern1.00pt\check{\ell} \leftarrow \big(4\cdot\gelm\cdot\sin(\alpha\mathbin{/}2)\big)\mathbin{/}3\,\alpha$\tcc*[r]{Eq:~\ref{eq:distToCoG}}
	\nl			$\kern1.00pt\zeta \leftarrow \gelm\mathbin{/}\sin(\beta)$\tcc*[r]{Eq:~\ref{eq:zeta}}
	\nl			\For{$j \in {1,2}$}{
	\nl				$\tilde{\ell}_j \leftarrow \zeta\mathbin{/}\bE_j$\tcc*[r]{Eq:~\ref{eq:distanceIForInterpolation},~\ref{eq:distanceIp1ForInterpolation}}
	\nl				$f'_j \leftarrow f_0\cdot(1 - \tilde{\ell}_j) + f_j\cdot\tilde{\ell}_j$\tcc*[r]{Eq:~\ref{eq:interpolatedFi},~\ref{eq:interpolatedFip1}}
				}
	\nl			$\check{f} \leftarrow f_0\cdot(1 - \check{\ell}) + \big((f'_1 + f'_2)\cdot\check{\ell}\big)\mathbin{/}2$\tcc*[r]{Eq:~\ref{eq:weightedMeanAtCoGatSector}}
	\nl			$\kern-2.0pt\tilde{f}_v \leftarrow \tilde{f}_v + A\cdot\check{f}$\tcc*[r]{Eq:~\ref{eq:meanFuncValAtPv}}
	\nl			$\kern-4.0pt\tilde{A}_v \leftarrow \tilde{A}_v + A$\tcc*[r]{Eq:~\ref{eq:meanFuncValAtPv}}
			}

	\nl		$f'_v \leftarrow \tilde{f}_v\mathbin{/}\tilde{A}_v$\tcc*[r]{Eq:~\ref{eq:meanFuncValAtPv}}
		}
	}
	\caption{Serial algorithm for convolving \Forf{t}\label{alg:serialConvolveFilter}}
\end{algorithm}%
\nomenclature[pa]{$\tilde{f}$}{the total volume of function values over $\bO_v$}%
\nomenclature[pb]{$\tilde{A}$}{the area of $\bO_v$}%

As we leave this section, we move further away from the theoretical, and endeavor to address actual implementations and hardware/software specific considerations.\todoReword{elaborate to make longer}
\pagebreak

%
%
%
%
\section{Parallel Algorithms}
\label{ch5sPA}
\todoBackground{Data dependencies}
Now that we have three serial algorithms for \Forf{t}, let us proceed towards a parallel variant for each. We will be using CUDA extended C++ for when language specific considerations are required. First we inspect for loops, then we analyze for control and data dependencies.

%
%
\subsection{Discovering Neighborhoods}
\label{ch5sPAssDN}
Beginning with Algorithm~\ref{alg:serialBuildNeighborhoods}, line~\ref{sbn1}, we encounter a loop over each face $\bt$ in $\bT$. To determine if each iteration of the loop may be computed in parallel, an analysis of every operations in the loop block is required, which in this case, only includes lines~\ref{sbn2}-~\ref{sbn4}.

During each iteration of the loop, the program reads the global index for each of three points in the set $\bt$, then for each point, performs a union operation between the neighborhood with the matching index and its set of two neighbors. The entire family of sets $\bT$ is immutable, so no conflicts can ever arise from reading from any of its members in any order. Given the associative property of the union operation, we can split each line into two separate operations, one for each neighbor to be possibly added to the neighborhood. Furthermore, if the neighborhoods are implemented using the C++ standard container, ~\nameref{ch2sCECssCSLsssS}, uniqueness is already guaranteed, and any newly discovered neighbor may be inserted into its neighborhood set in any order, with out fear of duplication.

Figure~\ref{fig:parallelBuildNeighborhoods} describes a very simple mesh of just four points and two faces, similar to what is seen in Figure~\ref{fig:triangularFaces}. The two faces, $\bt_1$ and $\bt_2$, are colored in sand and coral color respectively. The arrows represent the insert operation of an index of a point into a neighborhood implemented as a set of indexes, and are colored to match the face from where the point had come. The two pairs of arrows pointing from $\bp_2$ to $\bN_3$ and $\bp_3$ to $\bN_2$ are specially colored teal to highlight the fact that these insert operations occur twice, originating one each from each of the faces, but because of the uniqueness property of a set, the duplicated inserts are totally inconsequential to the the final set of neighbors in either neighborhood. Also notice that insert is called twice on each neighborhood for every face which contains its center point. This realization will influence the design of the parallel algorithmm.

\tikzset{%
	>={Latex[width=2mm,length=2mm]},
	baseNode/.style = {rectangle, rounded corners,
		draw=black, fill=white, thick,
		minimum width=1cm, minimum height=1cm,
		text centered, font=\sffamily},
	baseLine/.style = {double, thick},
	tealStyle/.style = {draw=MyTeal, fill=MyLtTeal},
	coralStyle/.style = {draw=MyCoral, fill=MyLtCoral},
	sandStyle/.style = {draw=MySand, fill=MyLtSand},
	faceL/.style = {baseNode, sandStyle},
	lineL/.style = {baseLine, sandStyle},
	faceR/.style = {baseNode, coralStyle},
	lineR/.style = {baseLine, coralStyle},
	lineC/.style = {baseLine, tealStyle},
	point/.style = {baseNode},
	nbhd/.style = {baseNode, minimum width=2cm}
}
\begin{figure}[h]
	\begin{tikzpicture}[node distance=0cm]
		\coordinate (center1) at (0cm,0cm);
		\node (t1) [faceL, anchor=east, xshift=-.5cm] {$\bt_1 = \{\bp_1,\,\bp_2,\,\bp_3\}$};
		\node (t2) [faceR, anchor=west, xshift= .5cm] {$\bt_2 = \{\bp_3,\,\bp_2,\,\bp_4\}$};
		\coordinate (center2) at (0cm,-1.5cm);
		\node (p1) [point, left of=center2, xshift=-3cm] {$\bp_1$};
		\node (p2) [point, left of=center2, xshift=-1cm] {$\bp_2$};
		\node (p3) [point, right of=center2, xshift=1cm] {$\bp_3$};
		\node (p4) [point, right of=center2, xshift=3cm] {$\bp_4$};
		\coordinate (center3) at (0cm,-4.5cm);
		\node (n1) [nbhd, left of=center3, xshift=-5cm] {$\bN_1$};
		\node (n2) [nbhd, left of=center3, xshift=-1.75cm] {$\bN_2$};
		\node (n3) [nbhd, right of=center3, xshift=1.75cm] {$\bN_3$};
		\node (n4) [nbhd, right of=center3, xshift=5cm] {$\bN_4$};

		\draw[-, lineL] (t1) -- (p1);
		\draw[-, lineL] (t1) -- (p2);
		\draw[-, lineL] (t1) -- (p3);

		\draw[->, lineL] (p1) -- (n2);
		\draw[->, lineL] (p1) -- (n3);
		\draw[->, lineL] (p2) -- (n1);
		\draw[->, lineC] (p2) -- (n3.180);
		\draw[->, lineL] (p3) -- (n1);
		\draw[->, lineC] (p3) -- (n2);

		\draw[-, lineR] (t2) -- (p2);
		\draw[-, lineR] (t2) -- (p3);
		\draw[-, lineR] (t2) -- (p4);

		\draw[->, lineC] (p3) -- (n2.0);
		\draw[->, lineR] (p3) -- (n4);
		\draw[->, lineR] (p2) -- (n4);
		\draw[->, lineC] (p2) -- (n3);
		\draw[->, lineR] (p4) -- (n3);
		\draw[->, lineR] (p4) -- (n2);
	\end{tikzpicture}
	{\caption[Parallel Build Neighborhoods]{Parallel Build Neighborhoods.}\label{fig:parallelBuildNeighborhoods}}
\end{figure}

Unfortunately, the C++ std::set does not also guarantee thread safety. Therefore, as reading and writing to a neighborhood is a critical section of the algorithm, it must be explicitly protected using one of the methods discussed in \todoReference{semaphores and mutexes}. And while locking mechanisms can be expensive compute time and memory-wise, \todoCitation{how expensive are locking mechanisms}, because the ratio of points in $\bP$ to average neighborhood size\todoResearch{average neighborhood size} is so large, exploiting the concurrency is probably\todoResearch{quantify the cost} worth the cost, especially using a SIMT architecture, like is found in a GPGPU. Also, one can mitigate exactly half of the possible collisions for inserting into a neighborhood, by exploiting the fact that insert is called twice on each neighborhood for every face which contains its center point, and therefore, executing these two operations sequentially by design.

\begin{algorithm}[h]
	\DontPrintSemicolon
	\SetCommentSty{small}
	\SetKwFor{For}{for}{:}{}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}

	\Input{the set of all triangular faces $\bT$}
	\Output{the family of sets of discovered neighborhoods $\bN$}

	\bigskip
	\FuncSty{parallelBuildNeighborhoods}\FuncArgSty{($\bT$)}\;
\nl		$\mathfrak{M} \leftarrow \{\mu_1,\,\mu_2,\ldots,\,\mu_{|\bP|}\}$\;
\nl	\For(\tcc*[f]{$\bt = \left \{\bp_a, \bp_b, \bp_c\right \}$}){$\bt \in \bT$}{
\nl		\For{$i \in \{1,\,2,\,3\}$}{
			\linespread{1.5}\selectfont
\nl			$a,\enspace b,\enspace c \enspace\leftarrow\enspace i,\enspace (i+1)\%3,\enspace (i+2)\%3$\;
\nl			$\ProcSty{lock}(\mu_a)$\;
\nl			$\ProcSty{insert}(\bp_b) \Rightarrow \bN_a$\;
\nl			$\ProcSty{insert}(\bp_c) \Rightarrow \bN_a$\;
\nl			$\ProcSty{unlock}(\mu_a)$\;
		}
	}
	\caption{Parallel algorithm for building the family of sets of all discovered members of each neighborhood in the mesh\label{alg:parallelBuildNeighborhoods}}
\end{algorithm}%
\nomenclature[]{$\mathfrak{M}$}{a set of mutexes, $\mu_i$}%
\nomenclature[]{$\mu$}{a mutex}%

%
%
\subsection{Calculating Edge Lengths}
\label{ch5sPAssCEL}
Next, we examine the serial Algorithm~\ref{alg:serialCalculateEdgeLengths}, and already in line~\ref{scel1}, we encounter a loop over each point $\bp_v$ in $\bP$. In order to determine if each iteration of the loop may be computed in parallel, an analysis of every operation in the loop block is required, which in this case, includes lines~\ref{scel2}-~\ref{scel5}. The first internal line,~\ref{scel1}, starts another loop over each point $\bp_i$ in $\bN_v$. \todoReword{why important to know ?} As we have seen very clearly in Figure~\ref{fig:neighborhoods}, the cardinality of each individual neighborhood can not be predicted for irregular, triangle meshes, like those typical of acquired \tdd{}. However, that becomes less important because it is possible to know the total cardinality of the set of neighborhoods, by having counted the size of each neighborhood a posteriori. With that information, we may unroll the two loops and instruct a certain number of threads to process the loop block in parallel. If the average number of neighbors per point is six\todoResearch{average number of neighbors}, then this amount will be six times as large as the cardinality of $\pP$. Therefore we can allocate the memory to store $6x|\bP|$ lengths.

Line~\ref{scel3} of Algorithm~\ref{alg:serialCalculateEdgeLengths} is the $\ellstar$ operation, the most costly operation performed by \forf{t}, due to use of $\sqrt{(\cdot)}$. Therefore, unlike the insert operation in Algorithm~\ref{alg:parallelBuildNeighborhoods}, it is of paramount important that we avoid any duplication of of the $\ellstar$ operation.

%
%f{
%
%
\section[Acceleration by GPGPU]{Acceleration by General-purpose
computing on Graphics Processing Units (GPGPU)}

%
Data Partitioning:
Calculations depend on specific data structures.~\cite[p.~357]{Lang17}
As in edge lengths depend on the neighborhoods.

%
Data Dependencies: ~\cite[p.~358]{Lang17}
Section~\ref{ch2sACssCVP}
%
Functional Partitioning: ~\cite[p.~359]{Lang17}
different operations on the same data

%
%
%
\section{Summary}
%then calculate all the edge lengths $\ell_{vk}$, as well as the global minimum edge length $\gelm$. Afterwards, one may efficiently convolve the filter, for as many number of iterations as required to achieve the desired smoothing effect.

