\chapter{Conclusions}
As of this writing, the fastest available GPGPU card available is the Quadro RTX 6000 which has 4,608 parallel processing cores and can perform at 16.3 TFLOPS~\cite{quadro6k}, up from the the previous 5000 model, which already had 3,702 and could perform at 11.2 TFLOPS~\cite{quadro5k}.

The fastest CPU commercially available as of this writing is the Intel Core i9-9980XE, which has 16 cores which each operate at 3.0GHz, and costs about \$2,300.
\todoCitation{cpu benchmarks}
%https://www.cpubenchmark.net/cpu.php?cpu=Intel+Core+i9-9980XE+%40+3.00GHz&id=3373


%
%
%
%
%
%
\section{Summary}

The goal of our experimentation was two-fold, analyzing the filter response on various kinds of \tdd{} as well as evaluating the performance of the parallel variant of \fors{t} algorithm. Therefore, we began in Section~\ref{ch6sSTDD} by analyzing the filter response when convolving the filter of over four different configurations of synthetic \tdd{}: the bisected square tessellation, the quadrisected square tessellation, the hexagonal tessellation, and the random triangulated disc, each with the \gls{ddf} applied as a scalar field.

Figure~\ref{fig:sq2} exposed the behavior of the anisotropic filter response traveling faster along the connections of more distant adjacent points. Figures~\ref{fig:sq4}~and~\ref{fig:hex} show the filter response exhibiting isotropic behavior as the density of the mesh increased, with more adjacent points per unit compared to the bisected square tesselation, while also reducing the overall variance in the length of edges. Figures~\ref{fig:rdisc} showed how the filter response appears to slow down when convolved over irregular meshes, more similar to acquired \tdd{}.

Next, in Section~\ref{ch6sATDD} we analyzed the filter response when convolving the filter of over three different examples of acquired \tdd{}: the partial model of the University of Heidelberg seal, the flat surface from ILATO, and the Stanford Bunny, each processed having been processed by \gls{tMSIIf}.

In Figure~\ref{fig:unisiegel} an error in computation was seen propagating across the image with each subsequent convolution. However, as that error stems from convolving the filter over ``unclean'' \tdd{}, the solution lies not with the filter, but with first processing the data with another tool, such as the ``Automatic Mesh Polishing'' provided by the GigaMesh framework.

In Figure~\ref{fig:ILATO}, the filter response efficacy diminishes,  seemingly having had its effectiveness compromised by convolving the filter over a scalar field with very low variance, highlighting another complexity of working with acquired \tdd{}, where one solution may be to set limits on the range of values visualized by the software.


%
%
%
%
%
%
\section{Future Work}
What they are and why I did not.
\begin{itemize}
	\item Implement in OpenCL to include all GPUs
	\item Implement in OpenMP to use multiple machines
	\item Implement in PThreads to exploit MIMP
	\item Pipelining memory reads/calculations exploit more concurrency
	\item Edge case handling: max mesh size in memory, Derive calculation for compute time per iteration by mesh size. Maybe find when load time is greater than iteration time
	\item support other file types
	\item Calculating edge length takes longest, so DO NOT DOUBLE EFFORT HERE
	\item Determine is using $\elm$ vs $\bar{\elm}$ has any effect, especially on one-ring neighborhood with a relatively large $\elm$ on mesh with a very small $\bar{\elm}$
	\item More analysis on \fors. it is my intuition that the un-isotropic nature of the filter is due to the speed at which information travels along longer edges.
	\item Apply the filter to multi-channel vector fields like RGB, however color-wheel based methods may be better From scetion
\label{ch2sTDDssFV}
	\item Implement Median and Mode version of filter (others based on what's foudn in 2d filter results)
	\item Implement more storage vs speed options
	\item explore using the inner angles $\alpha_i$ in stead of area for weighting
	\item instead of global min size, just choose a size, especially if not using sqrt to get edgelengths
	\item create more synthetic meshes with different scalar fields, Random, etc
	\item \ref{fig:speedup} Another trend is that for most configurations, speedup increases with increasing number of convolutions, seeming to converge to a certain number. More research must be done in order to determine why this may be, but it is possible that it is related to low-level memory optimization by the operating system.
	\item if neighborhood sizes vary widely, one large neighborhood can cause all other threads to wait. therefore, build in work sharing for big neighborhoods.
	\item improve efficency of parallel algorithm
	\item exploit errorInSource to speed up parallel algorithm
	\item at end of \label{ch5sCELPssASACEL} Conversely, in order to save nearly half\footnote{Scaling by half comes from the observation that as mesh density increases, the ratio of border to non-border edges dimensions, which discussed in more detail Appendix~\ref{apdx1}.} of that memory, one could store the value only once by implementing the control structures for detecting if an edge length has already been saved, then when retrieving the values, one could search for the edge length required at the cost of compute time. In the next section, we choose to implement the first, speedier method.
\end{itemize}

