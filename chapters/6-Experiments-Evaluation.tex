\chapter{Experiments \& Evaluation}
\label{ch6}
The goal of our experimentation was two-fold. First, it was our intuition that it would be useful to visualize \Fors{t} response when convolved over both regular and irregular synthetic \tdd{} with the \gls{ddf} applied as a scalar field, and when convolved over acquired \tdd{} with the result of \gls{tMSIIf} applied as a scalar field. Secondly, the performance of the parallel algorithm was evaluated in regards to the speedup and efficiency obtained by convolving \fors{t} over meshes of different sizes, using different hardware configurations.

In Section~\ref{ch6sSTDD}, we analyze the filter response when convolving the filter over four different configurations of synthetic data, with each image of a mesh visualized using the GigaMesh~\cite{Mara10} framework, with function values colored using the ``Improved Hot'' color ramp, then exported as a raster image in the \gls{PNG} format. Next, in Section~\ref{ch6sATDD} we analyze the same for three different examples of acquired \tdd{}. Then, in Section~\ref{ch6sCWG}, we establish the compute times for a multitude of pairs of experiments, involving (a) both the serial and parallel algorithms, (b) both acquired and synthetic \tdd{}, and (c) four different configurations of hardware.

%
%
%
%
%
%
\section{Synthetic \tdd{}}
\label{ch6sSTDD}
In order to study the effect of each convolution of \fors{t} on individual function values of a scalar field, we generated synthetic \tdd{} arranged into four different configurations of triangle meshes, then applied the Dirac delta function as a scalar field\todoCitation{Dirac delta function}. This involves applying a function value of one at the center point, and zero everywhere else, before iteratively convolving \fors{t} multiple times over each mesh.

The four synthetic mesh configurations are: the bisected-square tessellation, the quadrisected-square tessellation, the hexagonal tessellation, and the random triangulated disc. These were generated using original C++ source code which was written specifically for the research presented in this thesis. However, as such tools may also be found useful by other researchers, it is our intention to make the source code for each generator as accessible as this thesis.

%
%
%
%
\pagebreak
\subsection{Bisected-Square Tessellations}
\label{ch6sSTDDssBST}
The synthetic mesh generator for bisected-square tessellations generates meshes characterized by rings of squares around a center point, with the northwest and southeast corners made adjacent, so as to bisect each square into two equally-sized isosceles, right triangles. This results in two dissimilar edge lengths adjacent to the center point.

The smallest, non-trivial mesh, generated with the parameter $r$ equal to one, is composed of four squares around the center point. These are represented in total by only nine points and eight faces. However those numbers grow quickly with increasing parameter size $r$, according to the two equations
\begin{align}
	|\bP| &= 4r^2 + 4r + 1 \\
	|\bT| &= 8r^2
	\label{eq:sq2PointAndFaceCounts}
\end{align}

Figure~\ref{fig:sq2} shows a comparison of two differently-sized, bisected-square tessellations, generated with parameters $r$ set to 1 and 10. Shown in (a) and (d) are each in wireframe. Next in (b) and (e), each is colored by function value before convolving the filter. Then, in (c) $r=1$ is shown after convolving the filter once. Finally in (f) $r=10$ is shown after convolving the filter 100 times. Notice how the filter response appears to travel faster across the image, along the longer, diagonal edge lengths.

\input{figures/latex/sq2.tex}

%
%
%
%
\pagebreak
\subsection{Quadrisected-Square Tessellations}
\label{ch6sSTDDssQST}
The synthetic mesh generator for quadrisected-square tessellations generates meshes characterized by rings of squares around a center point, with every corner made adjacent to the center point, quadrisecting each square into four equally-sized, isosceles, right triangles.

The smallest, non-trivial mesh, generated with the parameter $r$ equal to one, is composed of four squares around the center point, represented in total by only thirteen points and sixteen faces. However those numbers grow quickly with increasing parameter size $r$, according to the two equations
\begin{align}
	|\bP| &= 8r^2 + 4r + 1 \\
	|\bT| &= 16r^2
	\label{eq:sq4PointAndFaceCounts}
\end{align}

Figure~\ref{fig:sq4} shows a comparison of two differently-sized, quadrisected-square tessellations, generated with parameters $r$ set to 1 and 10. Shown in (a) and (d) are each in wireframe. Next in (b) and (e), each is colored by function value before convolving the filter. Then in (c) $r=1$ is shown after convolving the filter once. Finally in (f) $r=10$ is shown after convolving the filter 100 times. Notice the circular shape of the filter response in part (f).

\input{figures/latex/sq4.tex}

%
%
%
%
\pagebreak
\subsection{Hexagonal Tesselation}
\label{ch6sSTDDssHT}
The synthetic mesh generator for hexagonal tessellations generates meshes characterized by hexagons around one central hexagon, with each corner of the hexagons made adjacent to its own center point, creating six equilateral triangles per hexagon, and ensuring that every edge length in the mesh is of equal size.

The smallest, non-trivial mesh, generated with the parameter $r$ equal to zero, is composed of one hexagon around the center point, represented in total by only seven points and six faces. However those numbers grow quickly with increasing parameter size $r$, according to the two equations
\begin{align}
	|\bP| &= 3r^2 + 3r + 7 +\sum_{i=1}^r{6(2i + 1)}\\
	|\bT| &= 18r^2 + 18r + 6
	\label{eq:hexPointAndFaceCounts}
\end{align}

Figure~\ref{fig:sq2} shows a comparison of two differently-sized, hexagonal tessellations, generated with parameters $r$ set to 1 and 10. Shown in (a) and (d) are each in wireframe. Next in (b) and (e), each are colored by function value before convolving the filter. Then, in (c) $r=1$ is shown after convolving the filter twice. Finally in (f) $r=10$ is shown after convolving the filter 200 times.

\input{figures/latex/hex.tex}

%
%
%
%
\pagebreak
\subsection{Random Triangulated Discs}
\label{ch6sSTDDssRTD}
The synthetic mesh generator for random triangulated discs generates meshes characterized by radially bounded, uniformly distributed, random points, and triangulated following the well-studied method originally presented by Delaunay~\cite{Delaunay34}. Generating random triangulated discs requires two parameters: the radius of the disc $r$, and the number of points $p$. These parameters were chosen to ensure parity with the sizes and point density of the other synthetic meshes, as summarized in Table~\ref{tbl:rdisc}.
\vspace*{-.5\baselineskip}
\begin{table}[ht]
\begin{tabular}{rrr|rrr}
\textbf{Radius $r$} & \textbf{Points $p$} & \textbf{Faces} & \textbf{Radius $r$} & \textbf{Points $p$} & \textbf{Faces}\\
\hline
    1 &         11 &          12 &   100 &     60,401 &     120,668\\
    3 &         67 &         119 &   300 &    541,201 &   1,082,132\\
   10 &        641 &       1,252 & 1,000 &  6,004,001 &  12,007,382\\
   30 &      5,521 &      10,971 & 3,000 & 54,012,001 & 108,022,714%
\caption{Summary of the parameters used with the random triangulated disc generator\label{tbl:rdisc}}
\end{tabular}
\end{table}

Figure~\ref{fig:sq2} shows a comparison of two differently-sized, random triangulated discs, generated with parameters $r$ set to 1 and 10, and parameters $p$ set to 11 and 641. Shown in (a) and (d) are each in wireframe. Next in (b) and (e), each are colored by function value before convolving the filter. Then in (c) $r=1$ is shown after convolving the filter twice. Finally in (f) $r=10$ is shown after convolving the filter 10,000 times.

\vspace*{-\baselineskip}
\input{figures/latex/rdisc.tex}

%
%
%
%
%
%
\pagebreak
\section{Acquired \tdd{}}
\label{ch6sATDD}
Each of the three acquired \tdd{} examples used in our experiments were initially processed with the GigaMesh framework in order to generate a scalar field of function values using the Multi-Scale Integral Invariants filter, MSII~\cite{Mara12}. The examples we use are the partial model of the University of Heidelberg seal, the flat surface from ILATO, and the Stanford Bunny. Also, we experience two kinds of difficulties, error propagation and diminishing effectiveness when convolving the filter. These highlight the complexity of working with acquired \tdd{}.

%
%
%
%
\subsection{The University of Heidelberg Seal}
\label{ch6sATDDssUHS}
This acquired \tdd{} is a partial model, comprised of 397,210 points and 789,406 faces, taken from the center of a 3D-scan of the seal of the University of Heidelberg, as established in 1386 and stored by the Univeristy of Heidelberg Archives. The original data was captured with a high resolution 3D-scanner and is stored in the heiDATA dataverse of the IWR Computer Graphics~\cite{Unisiegel}.

Figure~\ref{fig:unisiegel} shows three views of the partial model taken from the University of Heidelberg seal: (a) in wireframe, (b) colored by function values generated by applying \gls{tMSIIf} before convolving \fors{t}, and (d) colored by function value after convolving \fors{t} 100 times.

\input{figures/latex/unisiegel.tex}

There exist several holes in this \tdd{}, including the relatively large hole in the lap of the figure, just below the center of each image. However, as \fors{t} convolves over every point, considering each neighborhood in isolation, the convexity of the entire mesh is inconsequential, and faces adjacent to holes are treated the same as faces adjacent to borders, where \wmfv{s} are averaged over the sectors of the geodesic disc which do exist.

Also, notice the large black area in the top left corner of (c). Due to one of the many non-manifold irregularities of the original mesh, an error occurred when the filter attempted to divide a function value by a zero-sized area, and that error then propagated to every adjacent neighborhood in every subsequent iteration. Using the ``Automatic Mesh Polishing'' feature in GigaMesh~\cite[p.~29-32]{Mara12}~\cite[p.7]{Giga17}, before convolving the filter, completely eliminates the problem. However further modifications to \fors{t} algorithm may be necessary to ensure the overall stability of the filter.

%
%
%
%
\subsection{A flat surface from \gls{ILATO}}
\label{ch6sATDDssI}
This acquired \tdd{} is a partial model, comprised of 56,215 points and 111,311 faces, taken from the center of sample 1A of the set of industrial samples presented by the \gls{ILATO} project~, Improving Limited Angle computed Tomography by Optical data integration\cite{ILATO14}. The original data was obtained by scanning a brushed-surface-finish aluminum block, with a Breuckmann scanners SmartScan 3D~\cite{Bayer16}.

Figure~\ref{fig:ILATO} shows four views of the partial model taken from \gls{ILATO} sample 1a: (a) in wireframe, (b) colored by function values generated by applying the MSII filter, before convolving \fors{t}, (c) colored by function value after convolving \fors{t} 1000 times, and then (d) again after convolving the filter 3000 times.
\input{figures/latex/ILATO.tex}

As with the other acquired \tdd{}, the function values were computed by applying \gls{tMSIIf} and use the same color ramp as the other models, however, because total variance over the entire model is very small, in the range of -0.018 and 0.018, practically representing the noise characteristic of the 3D-scanner and software used to generate the mesh. The range of function values are also compressed, so smoothing does not occur as rapidly as seen in other experiments, which is expected.

%
%
%
%
\subsection{The Stanford Bunny}
\label{ch6sATDDssSB}
The Stanford Bunny was range scanned in 1994 using a Cyberware 3030MS optical triangulation scanner. The ten separate scans were then zippered together~\cite{Turk94} to produce the \tdd{}, which we obtained from The Stanford 3D Scanning Repository~\cite{StanfordBun}. This mesh is comprised of 35,947 points and 69,451 faces.

Figure~\ref{fig:bun} shows three views of the Stanford Bunny: (a) in wireframe, (b) colored by function values generated by applying \gls{tMSIIf}, before convolving \fors{t}, and (c) colored by function value after convolving \fors{t} 100 times.
\input{figures/latex/bun.tex}

In this section, we presented three examples of acquired \tdd{} and experienced some of the difficulties inherent to processing and analyzing irregular triangle meshes. Without first cleaning the mesh, we experience error propagation with the university seal, and because of the relatively small variance in the function values of the flat surface, we experience a slow down in the effectiveness of the filter. In the next section we present and evaluate the results of the multitude of experiments conducted in order to establish the performance of the parallel variant of \Fors{t} algorithm.

%
%
%
%
%
%
\section{Convolving with a GPGPU}
\label{ch6sCWG}
In this section, we present the collection of experiments performed with the goal of establishing the speedup and efficiency obtained when convolving \Fors{t} utilizing the \gls{SIMD} concurrency architecture provided by \glspl{GPGPU}. To that end, first we will present the optimal, sequential execution time, and the parallel compute time for the two variants of the \fors{t} algorithm, as described in Section~\ref{ch6sCWGssCT}. Then we will analyze the speedup obtained for each pair of experiments in Section~\ref{ch6sCWGssS}. Next we will analyze the efficiency of each pair of experiments in Section~\ref{ch6sCWGssE}. Finally, we conclude with a discussion of the efficacy of our implementation of the parallel variant of \fors{t} algorithm.

%
%
%
%
\subsection{Compute Times}
\label{ch6sCWGssCT}
Before we are able to evaluate the \gls{speedup} and \gls{efficiency} of the parallel variant of \Fors{t}, as discussed in Section~\ref{ch2sPPssEAPA}, we must first obtain two essential timings: the optimal sequential execution time $\mathit{T_s}$, and the parallel compute time $\mathit{T_{\rho}}(\hat{n},\,\rho)$. In order to establish these values, we generated four versions each of the synthetic, bisected-square tessellation and the synthetic, hexagonal tessellation, using the members of the set \{1, 10, 100, 1000\} as parameter $r$. Combined with the two acquired meshes, the university seal and the flat surface, these totaled ten meshes ranging in sizes from just nine points and eight faces, to $9\times 10^6$ points and $1.8\times 10^7$ faces.

Next, in order to obtain comparisons across hardware of differing capabilities, we designated two machines: a laptop M, and a desktop T. Each of these has both a CPU for serial computation, and a GPU for parallel processing. T operates on an Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz, while M operates on an Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz. T has installed an NVIDIA Corporation GP104GL [Quadro P5000] GPU with 2,560 CUDA cores~\cite{quadro5k}, while M has installed an NVIDIA Corporation GP107M [GeForce GTX 1050 Ti Mobile] with 768 CUDA cores~\cite{geforce1050}.

Then, we systematically executed every combination\footnote{Of the 320 combinations of experiments, we were only able to record data for 309 different combinations because of the memory limitations of the laptop M. However, given the strong trends already established, it is uncertain that any additional data would influence the conclusions we draw about the reported experiments.} of hardware and mesh for eight increasing amounts of convolutions, \{1, 3, 10, 30, 100, 300, 1000, 3000\}. We restarted the count for each experiment in order to ensure that the resulting time was genuine.

For the serial algorithm, we used the implementation existing in the GigaMesh\todoCitation{gigamesh} framework, for the parallel algorithm, a new implementation in CUDA extended C++\todoCitation{cuda in C++} was developed and used. Verification of final resulting scalar field of \wmfv{s} were conducted at the conclusion of each pair of experiments, and it was determined that zero difference existed between the output of the serial algorithm versus the parallel variant, even with the largest meshes for the most convolutions.

Figure~\ref{fig:computeTimesLP} shows the timing results of every recorded experiment. The graph is drawn extra tall to ensure that the grid remains square, for an unbiased assessment of the correlations therein. Results from each combination of hardware and mesh are connected by lines.
%\todoResearch{add formula for timing (or at least ref to it) here.}
%\todoResearch{process time noise at very fast speeds}

\begin{figure}[ht]
\includegraphics[width=1.0\linewidth,height=1.0\textheight,keepaspectratio]
	{figures/computeTimesLinespoints.png}
	\caption[Compute Times per Experiment for increasing Convolution Counts]{Compute times per experiment, for increasing numbers of convolutions of \fors{t}, onto acquired and synthetic \tdd{} of varying sizes. Combinations involving the smallest mesh sizes are colored red, the largest mesh sizes in blue, with all others scaled in between. The experiments involving acquired \tdd{} are specially marked with squares for the flat surface mesh, and circles for the university seal.

\vspace*{\baselineskip}
\scriptsize† The experiment codes used correspond to: the system, T for the desktop or M for the laptop; the algorithm variant, P for parallel or S for serial; the mesh being processed, BS for bisected-square, HT for hexagonal tessellation, US for university seal, and FS for the flat surface; and finally, the count of points comprising the mesh.}
	\label{fig:computeTimesLP}
\end{figure}

Notice how the compute times increase linearly with both mesh size and number of iterations. Timing is very predictable when the total compute time is at least greater than 0.1 seconds, but is much less predictable for faster periods. Fortunately, the deviations from the trends only occur for short total timings, and do not appear to propagate per convolution for medium or long compute times. Therefore they are unlikely to be noticed by the typical user.

Figure~\ref{fig:computeTimesS} plots the total compute times of each experiment as the area of a circle\footnote{The areas of the circles were scaled by the cube root of the actual size. This is so that the smallest circle is still visible and the largest remains within the border of the plot, while the general growth trend is still recognizable.}, given different hardware and algorithm configurations, for each combination of mesh size, measured in point counts and number of convolutions of \fors{t}.

In general, one can see that for all experiments, the total compute times increase with the size of the mesh, as well as the number of convolutions. As a result, an increase in both produces in an exponential increase in the compute time, which is as expected given the complexity of the algorithm as discussed in Chapter~\ref{ch3}. This also demonstrates the positive effect of the parallel processing, as the growth of the compute times is much slower for the parallel variant than for the serial.

\begin{figure}[ht]
	\includegraphics[width=\linewidth]{figures/computeTimesScatter.png}
	\caption[Compute Times Scatter Plot]{A plot of the total compute times of each experiment as the area of a circle, given different hardware configurations. Each is then plotted for each combination of mesh size measured in point counts, and the number of convolutions of \Fors{t}. Blue circles indicate the serial variant computed with a \gls{CPU}, and a red circle indicates the parallel variant was computed on a \gls{GPGPU}.

\vspace*{\baselineskip}
\scriptsize† See Figure\ref{fig:computeTimesLP} for an explanation of the experiment codes.}
	\label{fig:computeTimesS}
\end{figure}

In this section, we endeavored to establish the optimal sequential execution time $\mathit{T_s}$, and the parallel compute time $\mathit{T_{\rho}}(\hat{n},\,\rho)$, given different hardware configurations and mesh sizes. In addition to the timing values, we also discovered a few trends characteristic of \Fors{t}. In the next section, we will continue our evaluation by calculating the speedup and efficiency obtained by processing the parallel variant of the filter algorithm using \glspl{GPGPU}.

%
%
%
%
\subsection{Speedup}
\label{ch6sCWGssS}
Having established the optimal sequential execution time $\mathit{T_s}$, and the parallel compute time $\mathit{T_{\rho}}(\hat{n},\,\rho)$ of \Fors{t}, we can now continue our evaluation by calculating, as discussed in detail in Section~\ref{ch2sPPssEAPA}, the \gls{speedup} obtained by convolving the filter over different sized meshes in parallel using \glspl{GPGPU}.

Figure~\ref{fig:speedup} is a plot of the \gls{speedup} obtained by convolving the parallel variant of \fors{t} over meshes of different sizes for increasing number of convolutions, using either the \gls{GPGPU} of the laptop M with 768 \gls{CUDA} cores or the \gls{GPGPU} of desktop T with 2,560 \gls{CUDA} cores.

\begin{figure}[ht]
	\includegraphics[width=\linewidth]{figures/speedup.png}
	\caption[Speedup]{A graph of the \gls{speedup} obtained by convolving the parallel variant of \Fors{t} over meshes of different sizes. Lines in blue, increasing in saturation with the size of the mesh, are experiments run using the \gls{GPGPU} of laptop M with 768 \gls{CUDA} cores. Lines in red, increasing in saturation with the size of the mesh, are experiments run using the \gls{GPGPU}  of desktop T with 2,560 \gls{CUDA} cores. The experiments involving acquired \tdd{} are specially marked with squares for the flat surface mesh, and circles for the university seal.

\vspace*{\baselineskip}
\scriptsize† See Figure~\ref{fig:computeTimesLP} for an explanation of the experiment codes.}
	\label{fig:speedup}
\end{figure}

One general trend which emerges is that, for both hardware configurations, more speedup can be gained when convolving the filter on meshes of larger sizes. This can be explained by the increase in the \gls{degreeOfPar}, which comes with meshes with large counts of points, and is related to \gls{Amdahl}, as discussed in Section~\ref{ch2sPPssEAPA}.

Also, in general, but especially for larger meshes, convolving the filter using the \gls{GPGPU} of the desktop T garners high speedup than the equivalent experiments using the laptop M hardware configuration. This is expected, because as the \gls{degreeOfPar} increases with mesh size, the higher number of \gls{CUDA} cores available to T allow it to process more computations in parallel than M. Though it never reaches the ratio of 2,560 to 768 that one might expect, as discussed in more detail in the next section, Section~\ref{ch6sCWGssE}.

Another trend that develops is that for most configurations, speedup increases with increasing number of convolutions, seeming to converge to a certain number. More research must be done to determine why this occurs, but it is possible that it is related to low-level memory optimization by the operating system.

A third trend, which may not be easily seen in the graph, is that for the smallest-sized meshes, the speedup is less than one, indicating a slow down in compute time. In fact, the minimum speedup obtained in our experimentation was with the synthetic, bisected-square tesselation generated with parameter $r=1$, consisting of only nine points, which has a speedup of $\approx$0.35 when convolving with the parallel filter on desktop T's \gls{GPGPU} 3,000 times. That equates with performing three times as slow as the equivalent serial experiment, and is likely due to the overhead incurred by the necessary usage of locking mechanisms and explicit thread synchronization in the parallel variant of the algorithm.

Fortunately, the speedup obtained by convolving the parallel variant of \Fors{t} using \glspl{GPGPU}, is overall quite high. The maximum speedup obtained by any experiment was approximately 204.43, which equates to the parallel variant performing 200 times faster than the serial algorithm. This speedup was obtained when convolving the parallel variant of the filter, for 3,000 convolutions, on the synthetic, quadrisected-square tesselation generated with parameter $r=1000$, consisting of more than four million points, using desktop T's \gls{GPGPU}.

Also notable, the acquired \tdd{} enjoyed appreciable speedup, though considerably less than the synthetic meshes of equivalent sizes. The flat surface mesh obtained the maximum speedups of only $\approx$60.25 with the laptop M's \gls{GPGPU}, and $\approx$102.74 when using desktop T's \gls{GPGPU}, while the bisected-square tessellation generated with parameter $r=100$ peaked at $\approx$103.67 with M, and $\approx$154.80 with T, despite being smaller. Likewise, the university seal obtained the maximum speedups of only $\approx$60.08 with the laptop M's \gls{GPGPU}, and $\approx$113.10 when using desktop T's \gls{GPGPU}, while the similarly-sized, quadrisected-square tessellation peaked at $\approx$115.62 with M, and $\approx$204.43 with T. It is unclear what caused these dependencies, but we speculate that it may be due to the wider variance in neighborhood size and function values present in \tdd{}, when compared to synthetic \tdd{}.

%
%
%
%
\subsection{Efficiency}
\label{ch6sCWGssE}
In this section, we evaluate the other important metric for analyzing the performance of a parallel algorithm, \gls{efficiency}, as discussed in detail in Section~\ref{ch2sPPssEAPA}. Having established the optimal sequential execution time $\mathit{T_s}$, and the parallel compute time $\mathit{T_{\rho}}(\hat{n},\,\rho)$ of \Fors{t}, \gls{efficiency} is calculated as \gls{speedup} divided by the number of processors used to obtain it.

Figure~\ref{fig:efficiency} is a plot of the \gls{efficiency} obtained by convolving the parallel variant of \fors{t} over meshes of different sizes for increasing numbers of convolutions, using either the \gls{GPGPU} of the laptop M with 768 \gls{CUDA} cores, or the \gls{GPGPU} of desktop T with 2,560 \gls{CUDA} cores.

\begin{figure}[ht]
	\includegraphics[width=\linewidth]{figures/efficiency.png}
	\caption[Efficiency]{A graph of the \gls{efficiency} obtained by convolving the parallel variant of \Fors{t} over meshes of different sizes. Lines in blue, increasing in saturation with the size of the mesh, are experiments run using the \gls{GPGPU} of laptop M with 768 \gls{CUDA} cores. Lines in red, increasing in saturation with the size of the mesh, are experiments run using the \gls{GPGPU} of desktop T with 2,560 \gls{CUDA} cores. The experiments involving acquired \tdd{} are specially marked with squares for the flat surface mesh and circles for the university seal.

\vspace*{\baselineskip}
\scriptsize† See Figure~\ref{fig:computeTimesLP} for an explanation of the experiment codes.}
	\label{fig:efficiency}
\end{figure}

The most noticeable trend in Figure~\ref{fig:efficiency} is that experiments run using the laptop M hardware configuration obtain higher efficiency than the equivalent experiments run using the desktop T hardware configuration. This trend is attributable to the design of the parallel algorithm, which is unable to scale effectively and utilize the increase in processor count provided by the \gls{GPGPU} of T.

Secondly, the efficiency obtained when convolving the filter on acquired \tdd{} is approximately only half of the efficiency obtained when convolving the filter on similarly-sized synthetic meshes. This is a direct consequence of the slower speedup for acquired \tdd{}, as calculated in the previous section.

The maximum efficiency recorded is $\approx$0.151. This was obtained when convolving \fors{t} 3,000 times on the bisected-square tessellation with four million points. This equates to only 15\% of the total processor effort effectively contributing to the final values.

The minimum efficiency recorded is $\approx$0.000136, and similar to the minimum speedup, was obtained when convolving \fors{t} 3,000 times on the bisected-square tessellation with only nine points. This equates to only .01\% of the total processor effort effectively contributing to the final values.

%
%
%
%
%
%
\clearpage
\section{Summary}
The goal of our experimentation was two-fold: analyzing the filter response on various kinds of \tdd{} and evaluating the performance of the parallel variant of \fors{t} algorithm. We began in Section~\ref{ch6sSTDD} by analyzing the filter response when convolving the filter over four different configurations of synthetic \tdd{}, each with the \gls{ddf} applied as a scalar field.

Some anisotropic behavior was exhibited by \fors{t} response, as it traveled faster along the connections of more distant adjacent points, while isotropic behavior was exhibited when the overall variance in the length of edges was reduced. The filter response also appeared to slow down when convolved over irregular meshes, which was more similar to acquired \tdd{}.

Next, in Section~\ref{ch6sATDD} we analyzed the filter response when convolving over three different examples of acquired \tdd{}. This highlighted the way an error in computation propagated across the image with each subsequent convolution, as well as the way the efficacy of the filter diminished, when convolving the filter over a scalar field with very low variance.

Then in Section~\ref{ch6sCWG}, we established the compute times for a multitude of pairs of experiments. These involved both the serial sand parallel algorithm, both acquired and synthetic \tdd{}, and four different configurations of hardware.

In the next chapter, we will combine everything that we have discovered through our research, to make conclusions about \Fors{t} in general, and its parallel variant utilizing \glspl{GPGPU}.
