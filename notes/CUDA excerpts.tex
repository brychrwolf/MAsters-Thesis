\subsection{CUDA: An interface for GPGPU implementation}
Graphical Processor Units, GPUs, are highly parallel, multithreaded, manycore
processors typically characterized by very high computational power and
tremendous memory bandwidth. A GPU is especially well-suited when used to
data-parallel computations, in which the same program is executed on many data
elements in parallel.~\cite[p.~1.1]{CUDA18}

Mainstream processor chips, both CPU and GPUs, are now parallel systems, and as
this parallelism continues to scale with Moore's law, the real challenge is to
develop application software that transparently scales its parallelism to
leverage the increasing number of processor cores. The CUDA parallel
programming model, introduced by NVIDIA in November of 2006, was specifically
designed to overcome this challenge.~\cite[p.~1.3]{CUDA18}

In fact, due to its inherent scalable programming model in which problems are
decomposed in a way that each block of thread\index{thread} can be scheduled on
any of the available multiprocessors within a GPU, in any order, concurrently
or sequentially, so that any compiled CUDA program, such as our implementation
of the MSII filter, can be executed on any number of multiprocessors, and only
the runtime system needs to know the physical multiprocessor
count.~\cite[p.~1.3]{CUDA18}

~~~~~~~~~

CUDA C extends C by allowing the programmer to define C functions, called
kernels, that, when called, are executed N times in parallel by N different
CUDA thread\index{thread}, as opposed to only once like regular C
functions.~\cite[p.~2.1]{CUDA18}

thread\index{thread} can be identified using a one-dimensional,
two-dimensional, or three-dimensional thread\index{thread} index, forming a
one-dimensional, two-dimensional, or three-dimensional block of
thread\index{thread}, called a thread\index{thread} block. This provides a
natural way to invoke computation across the elements in a domain such as a
vector, matrix, or volume. However, a kernel can be executed by multiple
equally-shaped thread\index{thread} blocks, so that the total number of
thread\index{thread} is equal to the number of thread\index{thread} per block
times the number of blocks. thread\index{thread} blocks are required to execute
independently: It must be possible to execute them in any order, in parallel or
in series. This independence requirement allows thread\index{thread} blocks to
be scheduled in any order across any number of cores as illustrated by Figure 5,
enabling programmers to write code that scales with the number of
cores.~\cite[p.~2.2]{CUDA18}

CUDA threads\index{thread} may access data from multiple memory spaces during
their execution as illustrated by Figure 7. Each thread\index{thread} has
private local memory. Each thread\index{thread} block has shared memory visible
to all thread\index{thread} of the block and with the same lifetime as the
block. All thread\index{thread} have access to the same global
memory.~\cite[p.~2.3]{CUDA18}

the CUDA programming model assumes that the CUDA thread\index{thread} execute
on a physically separate device that operates as a coprocessor to the host
running the C program. The CUDA programming model also assumes that both the
host and the device maintain their own separate memory spaces in DRAM,
referred to as host memory and device memory. Unified Memory provides managed
memory to bridge the host and device memory spaces. Managed memory is
accessible from all CPUs and GPUs in the system as a single, coherent memory
image with a common address space.~\cite[p.~2.4]{CUDA18}

The compute capability of a device is represented by a version number, also
sometimes called its "SM version". This version number identifies the features
supported by the GPU hardware and is used by applications at runtime to
determine which hardware features and/or instructions are available on the
present GPU. Devices with the same major revision number are of the same core
architecture. The major revision number is 7 for devices based on the Volta
architecture, 6 for devices based on the Pascal architecture, 5 for devices
based on the Maxwell architecture, 3 for devices based on the Kepler
architecture, 2 for devices based on the Fermi architecture, and 1 for devices
based on the Tesla architecture. Note: The compute capability version of a
particular GPU should not be confused with the CUDA version (e.g., CUDA 7.5,
CUDA 8, CUDA 9), which is the version of the CUDA software
platform.~\cite[p.~2.4]{CUDA18}

Kernels can be written using the CUDA instruction set architecture, called PTX,
which is described in the PTX reference manual. It is however usually more
effective to use a high-level programming language such as
C.~\cite[p.~3.1]{CUDA18}

Any PTX code loaded by an application at runtime is compiled further to binary
code by the device driver. This is called just-in-time compilation.
Just-in-time compilation increases application load time, but allows the
application to benefit from any new compiler improvements coming with each new
device driver. It is also the only way for applications to run on devices that
did not exist at the time the application was compiled, as detailed in
Application Compatibility. When the device driver just-in-time compiles some
PTX code for some application, it automatically caches a copy of the generated
binary code in order to avoid repeating the compilation in subsequent
invocations of the application. The cache - referred to as compute cache - is
automatically invalidated when the device driver is upgraded, so that
applications can benefit from the improvements in the new just-in-time compiler
built into the device driver.~\cite[p.~3.1.1.2]{CUDA18}

The front end of the compiler processes CUDA source files according to C++
syntax rules. Full C++ is supported for the host code. However, only a subset
of C++ is fully supported for the device code~\cite[p.~3.1.5]{CUDA18}
%
\subsection{CUDA C Runtime}%3.2. CUDA C Runtime}
There is no explicit initialization function for the runtime; it initializes
the first time a runtime function is called. During initialization, the runtime
creates a CUDA context for each device in the system (see Context for more
details on CUDA contexts). This context is the primary context for this device
and it is shared among all the host thread\index{thread} of the application. As
part of this context creation, the device code is just-in-time compiled if
necessary (see Just-in-Time Compilation) and loaded into device memory. This
all happens under the hood and the runtime does not expose the primary context
to the application.~\cite[p.~3.2.1]{CUDA18}

As mentioned in Heterogeneous Programming, the CUDA programming model assumes a
system composed of a host and a device, each with their own separate memory.
Kernels operate out of device memory, so the runtime provides functions to
allocate, deallocate, and copy device memory, as well as transfer data between
host memory and device memory. Device memory can be allocated either as linear
memory or as CUDA arrays. CUDA arrays are opaque memory layouts optimized for
texture fetching. They are described in Texture and Surface Memory. Linear
memory exists on the device in a 40-bit address space, so separately allocated
entities can reference one another via pointers, for example, in a binary
tree.~\cite[p.~3.2.2]{CUDA18}
%
\subsubsection{Explicit Synchronization}%3.2.5.5.3. Explicit Synchronization}
There are various ways to explicitly synchronize streams with each other.
cudaDeviceSynchronize() waits until all preceding commands in all streams of
all host thread\index{thread} have completed.

cudaStreamSynchronize()takes a stream as a parameter and waits until all
preceding commands in the given stream have completed. It can be used to
synchronize the host with a specific stream, allowing other streams to
continue executing on the device.

cudaStreamWaitEvent()takes a stream and an event as parameters (see Events
for a description of events)and makes all the commands added to the given
stream after the call to cudaStreamWaitEvent()delay their execution until the
given event has completed. The stream can be 0, in which case all the commands
added to any stream after the call to cudaStreamWaitEvent()wait on the event.

cudaStreamQuery()provides applications with a way to know if all preceding
commands in a stream have completed. To avoid unnecessary slowdowns, all these
synchronization functions are usually best used for timing purposes or to
isolate a launch or memory copy that is failing.
%
\subsection{Versioning and Compatibility}%3.3. Versioning and Compatibility}
There are two version numbers that developers should care about when developing
a CUDA application: The compute capability that describes the general
specifications and features of the compute device (see Compute Capability) and
the version of the CUDA driver API that describes the features supported by the
driver API and runtime.

In the driver header file, the version of the driver API is defined by variable CUDA\_VERSION. It allows developers to check whether their application requires a newer device driver than the one currently installed. This is important, because the driver API is backward compatible, meaning that applications, plug-ins, and libraries (including the C runtime) compiled against a particular version of the driver API will continue to work on subsequent device driver releases as illustrated in Figure 11. The driver API is not forward compatible, which means that applications, plug-ins, and libraries (including the C runtime) compiled against a particular version of the driver API will not work on previous versions of the device driver.

It is important to note that there are limitations on the mixing and matching of versions that is supported: Since only one version of the CUDA Driver can be installed at a time on a system, the installed driver must be of the same or higher version than the maximum Driver API version against which any application, plug-ins, or libraries that must run on that system were built.

All plug-ins and libraries used by an application must use the same version of
the CUDA Runtime unless they statically link to the Runtime, in which case
multiple versions of the runtime can coexist in the same process space. Note
that if nvcc is used to link the application, the static version of the CUDA
Runtime library will be used by default, and all CUDA Toolkit libraries are
statically linked against the CUDA Runtime.

All plug-ins and libraries used by an application must use the same version of
any libraries that use the runtime (such as cuFFT, cuBLAS, ...) unless
statically linking to those libraries.
%
\subsection{Hardware Implementation}%4. Hardware Implementation}
The NVIDIA GPU architecture is built around a scalable array of multithreaded
Streaming Multiprocessors (SMs). When a CUDA program on the host CPU invokes a
kernel grid, the blocks of the grid are enumerated and distributed to
multiprocessors with available execution capacity. The thread\index{thread} of
a thread\index{thread} block execute concurrently on one multiprocessor, and
multiple thread\index{thread} blocks can execute concurrently on one
multiprocessor. As thread\index{thread} blocks terminate, new blocks are
launched on the vacated multiprocessors.

A multiprocessor is designed to execute hundreds of thread\index{thread}
concurrently. To manage such a large amount of thread\index{thread}, it employs
a unique architecture called SIMT (Single-Instruction,
Multiple-thread\index{thread}) that is described in SIMT Architecture. The
instructions are pipelined to leverage instruction-level parallelism within a
single thread\index{thread}, as well as thread\index{thread}-level parallelism
extensively through simultaneous hardware multithreading as detailed in
Hardware Multithreading. Unlike CPU cores they are issued in order however and
there is no branch prediction and no speculative execution.
%
\subsubsection{SIMT Architecture}%4.1. SIMT Architecture}
The multiprocessor creates, manages, schedules, and executes
thread\index{thread} in groups of 32 parallel thread\index{thread} called
warps. Individual thread\index{thread} composing a warp start together at the
same program address, but they have their own instruction address counter and
register state and are therefore free to branch and execute independently. The
term warp originates from weaving, the first parallel thread\index{thread}
technology. A half-warp is either the first or second half of a warp. A
quarter-warp is either the first, second, third, or fourth quarter of a warp.
When a multiprocessor is given one or more thread\index{thread} blocks to
execute, it partitions them into warps and each warp gets scheduled by a warp
scheduler for execution. The way a block is partitioned into warps is always
the same; each warp contains thread\index{thread} of consecutive, increasing
thread\index{thread} IDs with the first warp containing thread\index{thread}
0. thread\index{thread} Hierarchy describes how thread\index{thread} IDs
relate to thread\index{thread} indices in the block.
A warp executes one common instruction at a time, so full efficiency is
realized when all 32 thread\index{thread} of a warp agree on their execution
path. If thread\index{thread} of a warp diverge via a data-dependent
conditional branch, the warp executes each branch path taken, disabling
thread\index{thread} that are not on that path. Branch divergence occurs only
within a warp; different warps execute independently regardless of whether
they are executing common or disjoint code paths.
The SIMT architecture is akin to SIMD (Single Instruction, Multiple Data)
vector organizations in that a single instruction controls multiple processing
elements. A key difference is that SIMD vector organizations expose the SIMD
width to the software, whereas SIMT instructions specify the execution and
branching behavior of a single thread\index{thread}. In contrast with SIMD
vector machines, SIMT enables programmers to write thread\index{thread}-level
parallel code for independent, scalar thread\index{thread}, as well as
data-parallel code for coordinated thread\index{thread}. For the purposes of
correctness, the programmer can essentially ignore the SIMT behavior; however,
substantial performance improvements can be realized by taking care that the
code seldom requires thread\index{thread} in a warp to diverge. In practice,
this is analogous to the role of cache lines in traditional code: Cache line
size can be safely ignored when designing for correctness but must be
considered in the code structure when designing for peak performance. Vector
architectures, on the other hand, require the software to coalesce loads
into vectors and manage divergence manually.
%
\subsection{Host Memory and Device Memory}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi tincidunt eget
ipsum eu iaculis. Cras vel sem eu velit eleifend porta vel sit amet massa. Etiam
a posuere nunc. Aenean aliquam viverra dapibus. Aliquam ac eros a purus feugiat
rhoncus. Donec faucibus ut nibh ut cursus. Aliquam erat volutpat. Proin efficitur
nulla sit amet iaculis condimentum. Cras placerat leo vitae venenatis feugiat. In
hac habitasse platea dictumst. Orci varius natoque penatibus et magnis dis
parturient montes, nascetur ridiculus mus. In aliquet sagittis dui eu pulvinar.
Morbi a arcu eu dolor sagittis varius. Aliquam dignissim tortor sed tortor
suscipit, eget imperdiet mauris convallis.~\cite[p.~00]{todoCitation}\todoCitation
%
\subsection{Versions <9, vs >= 9}
K.2.1.1. Explicit Allocation Using cudaMallocManaged()~\cite[p.~272]{CUDA18}
Explain multi-threading model differences Motivation for choosing earlier
version Adjacent vertices are stored in an array of variable-length arrays.
First idea was determine max "width" of 2nd dimension, then to put the adjacent
vertices into a 2D array with constant width. The pros were that a single index
(i.e. CUDA) would then be able to now read the data.  The cons were that there
were lots of allocated memory never to be used, if a single vertex had a large
number of adjacenet pairs, it would greatly affect the size. Also, another
array of counts of pairs per vertex is needed, so that only memory containing
pair data was processesed per any given vertex. Instead, we can first create a
single array of "run lengths," which is for each vertex, the count of how many
adjacent pairs it has. Then flatten the array into a single dismension, using
the run-length array as the iterator. Pros are that only a single other area is
required, and the original array does not need to be padded with extra, unsued
memory address.
